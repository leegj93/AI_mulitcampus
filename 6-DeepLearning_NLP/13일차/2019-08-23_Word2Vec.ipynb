{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim(word2vec 라이브러리 주소)\n",
    "# https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자연어 처리 연습\n",
    "# https://www.kaggle.com/aashita/nyt-comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0823 09:12:33.200891  9040 deprecation_wrapper.py:119] From C:\\Users\\user\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0823 09:12:33.216251  9040 deprecation_wrapper.py:119] From C:\\Users\\user\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#model.add(SimpleRNN(hidden_size))\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "model=Sequential()\n",
    "model.add(Dense(3, input_dim=4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0823 09:20:00.127804  9040 deprecation_wrapper.py:119] From C:\\Users\\user\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0823 09:20:00.144692  9040 deprecation_wrapper.py:119] From C:\\Users\\user\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#FC FFNN\n",
    "from keras.layers import Input,Dense\n",
    "from keras.models import Model\n",
    "\n",
    "inputs=Input(shape=(10,)) #입력: 10개\n",
    "hid1=Dense(32, activation='relu')(inputs)\n",
    "hid2=Dense(32, activation='relu')(hid1)\n",
    "output=Dense(1,activation='sigmoid')(hid2)\n",
    "model=Model(inputs=inputs, outputs=output)\n",
    "\n",
    "model.compile(optimizer='rmsprop', \n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#model.fit(데이터, y레이블)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNN \n",
    "\n",
    "inputs=Input(shape=(30,1)) #30:time-step\n",
    "lstm_layer=LSTM(10)(inputs)\n",
    "x=Dense(10, activation='relu')(lstm_layer)\n",
    "output=Dense(1, activation='sigmoid')(x)\n",
    "Model(inputs=inputs, outputs=output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_3 (SimpleRNN)     (None, 3)                 42        \n",
      "=================================================================\n",
      "Total params: 42\n",
      "Trainable params: 42\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(3, input_shape=(2, 10))`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import SimpleRNN\n",
    "model=Sequential()\n",
    "#model.add(SimpleRNN(3, input_shape=(2,10)))\n",
    "model.add(SimpleRNN(3, input_length=2,input_dim=10))\n",
    "#2:input_length(time-step), 10:input_dim\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_2 (SimpleRNN)     (8, 3)                    42        \n",
      "=================================================================\n",
      "Total params: 42\n",
      "Trainable params: 42\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(SimpleRNN(3, batch_input_shape=(8,2,10)))\n",
    "#8:batch_size, 2:time_step, 10:input dim\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "timesteps=10 #시점, 문장을 구성하는 단어의 길이, 단어를 구성하는 글자의 수\n",
    "#주식예측모델에서 timesteps=7 => 일주일간의 주식변동패턴->오늘 주식을 예측\n",
    "input_dim=4 #단어가 4개인 벡터(코퍼스)\n",
    "hidden_size=8 #은닉상태의 크기(셀)\n",
    "inputs=np.random.random((timesteps, input_dim))\n",
    "inputs\n",
    "#초기 은닉 상태를 0으로 초기화\n",
    "hidden_state_t=np.zeros((hidden_size,))\n",
    "# ht= tanh(wx * xt + wh * ht-1 + b)\n",
    "\n",
    "#가중치\n",
    "#wx : 입력에 대한 가중치\n",
    "#wx:(은닉상태의크기 * 입력의 차원)\n",
    "wx=np.random.random((hidden_size,input_dim)) #(8,4)\n",
    "#wh : 은닉상태 가중치\n",
    "#wh:(은닉상태의크기 * 은닉상태의크기)\n",
    "wh=np.random.random((hidden_size,hidden_size)) #(8,8)\n",
    "#b:은닉상태의크기\n",
    "b=np.random.random((hidden_size,)) #8\n",
    "print(np.shape(wx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9699015991151154, 0.8469624479500694, 0.9078101247930693, 0.9711643450010612, 0.7084138956051698, 0.7938061718165186, 0.9548568363629657, 0.9872509124018891], [0.999996882908624, 0.9999745128778814, 0.9999946663513342, 0.9997366269787729, 0.9993944475553241, 0.9996927273537408, 0.9998669691891637, 0.9999965002412734], [0.9999971934728568, 0.999974819763604, 0.9999936128727844, 0.999897014632888, 0.9995718559339523, 0.9997584761263274, 0.9998197680837357, 0.999992791045098], [0.9999982808168211, 0.9999685806301147, 0.9999948283144887, 0.999901085899009, 0.999511647909654, 0.9998082474148013, 0.9998323183896362, 0.9999944906911751], [0.9999975727824698, 0.9999489878738224, 0.9999945121538395, 0.9997880995241415, 0.9992269543401276, 0.9998075119569433, 0.9997003627064349, 0.9999916004948396], [0.9999993046175203, 0.9999911253002638, 0.9999977785471381, 0.9999611859564085, 0.9997695743410651, 0.9998680305648276, 0.999957000698467, 0.999998292880873], [0.9999909605357459, 0.9999497236764054, 0.9999846257413085, 0.9997278700224687, 0.9993665657945731, 0.9996253619992072, 0.9994771881833803, 0.9999813396680256], [0.9999978078963132, 0.999980695597368, 0.9999966331642185, 0.9998136002948695, 0.9995139603655389, 0.9998341730715723, 0.9998542244868679, 0.9999952552656163], [0.9999980229243006, 0.999968119475954, 0.9999938798966599, 0.999904590678019, 0.999533910395501, 0.9997891299988718, 0.9998221600341479, 0.9999939766679128], [0.9999986502685363, 0.9999793374229239, 0.9999966507297313, 0.9998734237209308, 0.999567261527909, 0.999853998061381, 0.9998847711554412, 0.9999967105350007]]\n"
     ]
    }
   ],
   "source": [
    "total_hidden_states=[]\n",
    "#메모리 셀 동작\n",
    "for input_t in inputs:\n",
    "    output_t=np.tanh(np.dot(wx, input_t)+np.dot(wh,hidden_state_t)+b)\n",
    "    #wx*wt+wh*ht-1+b\n",
    "    total_hidden_states.append(list(output_t))\n",
    "    hidden_state_t=output_t\n",
    "print(total_hidden_states)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"멀티캠퍼스에 있는 교육생들이 공부하고 있다\\n\n",
    "교육생들은 딥러닝을 하고있다\\n\n",
    "결석한 교육생도 있고 지각하는 교육생도 있다\\n\n",
    "\"\"\"\n",
    "#\n",
    "xdata                y\n",
    "멀티캠퍼스에         있는\n",
    "멀티캠퍼스에  있는   교육생들이\n",
    "...\n",
    "멀티캠퍼스에, 3   => 있는 교육생들이 공부하고\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 4, 5, 6, 1, 7, 8, 9, 10, 2, 11, 12, 2, 1]]\n",
      "{'있다': 1, '교육생도': 2, '멀티캠퍼스에': 3, '있는': 4, '교육생들이': 5, '공부하고': 6, '교육생들은': 7, '딥러닝을': 8, '하고있다': 9, '결석한': 10, '있고': 11, '지각하는': 12}\n"
     ]
    }
   ],
   "source": [
    "from keras_preprocessing.text import Tokenizer\n",
    "t=Tokenizer()\n",
    "t.fit_on_texts([text])\n",
    "print(t.texts_to_sequences([text]))\n",
    "print(t.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "[[3, 4], [3, 4, 5], [3, 4, 5, 6], [3, 4, 5, 6, 1], [7, 8], [7, 8, 9], [10, 2], [10, 2, 11], [10, 2, 11, 12], [10, 2, 11, 12, 2], [10, 2, 11, 12, 2, 1]]\n"
     ]
    }
   ],
   "source": [
    "vocab_size=len(t.word_index)+1 #단어집합 크기 : 13\n",
    "# 입력:멀티캠퍼스, 단어수:2\n",
    "# 출력: 있는 교육생들이 \n",
    "\n",
    "sequences=list()\n",
    "for line in text.split(\"\\n\"):\n",
    "    #print(t.texts_to_sequences([line])[0])\n",
    "    encoded=t.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(encoded)):\n",
    "        sequence=encoded[:i+1]\n",
    "        sequences.append(sequence)\n",
    "print(len(sequences))        \n",
    "print(sequences)\n",
    "\n",
    "max_len=max(len(s) for s in sequences) #6\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "sequences=pad_sequences(sequences, maxlen=max_len, padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=sequences[:, :-1]\n",
    "y=sequences[:, -1]\n",
    "x\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=to_categorical(y, num_classes=vocab_size)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0823 13:42:19.576702  9040 deprecation.py:323] From C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0823 13:42:20.025392  9040 deprecation_wrapper.py:119] From C:\\Users\\user\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      " - 0s - loss: 2.5862 - acc: 0.0000e+00\n",
      "Epoch 2/300\n",
      " - 0s - loss: 2.5743 - acc: 0.0000e+00\n",
      "Epoch 3/300\n",
      " - 0s - loss: 2.5643 - acc: 0.0000e+00\n",
      "Epoch 4/300\n",
      " - 0s - loss: 2.5539 - acc: 0.0000e+00\n",
      "Epoch 5/300\n",
      " - 0s - loss: 2.5432 - acc: 0.0909\n",
      "Epoch 6/300\n",
      " - 0s - loss: 2.5322 - acc: 0.1818\n",
      "Epoch 7/300\n",
      " - 0s - loss: 2.5210 - acc: 0.1818\n",
      "Epoch 8/300\n",
      " - 0s - loss: 2.5096 - acc: 0.2727\n",
      "Epoch 9/300\n",
      " - 0s - loss: 2.4978 - acc: 0.2727\n",
      "Epoch 10/300\n",
      " - 0s - loss: 2.4856 - acc: 0.2727\n",
      "Epoch 11/300\n",
      " - 0s - loss: 2.4730 - acc: 0.3636\n",
      "Epoch 12/300\n",
      " - 0s - loss: 2.4600 - acc: 0.3636\n",
      "Epoch 13/300\n",
      " - 0s - loss: 2.4465 - acc: 0.3636\n",
      "Epoch 14/300\n",
      " - 0s - loss: 2.4324 - acc: 0.3636\n",
      "Epoch 15/300\n",
      " - 0s - loss: 2.4177 - acc: 0.3636\n",
      "Epoch 16/300\n",
      " - 0s - loss: 2.4024 - acc: 0.3636\n",
      "Epoch 17/300\n",
      " - 0s - loss: 2.3865 - acc: 0.3636\n",
      "Epoch 18/300\n",
      " - 0s - loss: 2.3700 - acc: 0.3636\n",
      "Epoch 19/300\n",
      " - 0s - loss: 2.3529 - acc: 0.3636\n",
      "Epoch 20/300\n",
      " - 0s - loss: 2.3351 - acc: 0.3636\n",
      "Epoch 21/300\n",
      " - 0s - loss: 2.3168 - acc: 0.3636\n",
      "Epoch 22/300\n",
      " - 0s - loss: 2.2979 - acc: 0.3636\n",
      "Epoch 23/300\n",
      " - 0s - loss: 2.2786 - acc: 0.3636\n",
      "Epoch 24/300\n",
      " - 0s - loss: 2.2589 - acc: 0.3636\n",
      "Epoch 25/300\n",
      " - 0s - loss: 2.2390 - acc: 0.3636\n",
      "Epoch 26/300\n",
      " - 0s - loss: 2.2189 - acc: 0.3636\n",
      "Epoch 27/300\n",
      " - 0s - loss: 2.1989 - acc: 0.3636\n",
      "Epoch 28/300\n",
      " - 0s - loss: 2.1789 - acc: 0.3636\n",
      "Epoch 29/300\n",
      " - 0s - loss: 2.1592 - acc: 0.3636\n",
      "Epoch 30/300\n",
      " - 0s - loss: 2.1398 - acc: 0.3636\n",
      "Epoch 31/300\n",
      " - 0s - loss: 2.1208 - acc: 0.3636\n",
      "Epoch 32/300\n",
      " - 0s - loss: 2.1023 - acc: 0.3636\n",
      "Epoch 33/300\n",
      " - 0s - loss: 2.0841 - acc: 0.3636\n",
      "Epoch 34/300\n",
      " - 0s - loss: 2.0664 - acc: 0.3636\n",
      "Epoch 35/300\n",
      " - 0s - loss: 2.0488 - acc: 0.3636\n",
      "Epoch 36/300\n",
      " - 0s - loss: 2.0314 - acc: 0.3636\n",
      "Epoch 37/300\n",
      " - 0s - loss: 2.0141 - acc: 0.3636\n",
      "Epoch 38/300\n",
      " - 0s - loss: 1.9965 - acc: 0.3636\n",
      "Epoch 39/300\n",
      " - 0s - loss: 1.9788 - acc: 0.3636\n",
      "Epoch 40/300\n",
      " - 0s - loss: 1.9607 - acc: 0.3636\n",
      "Epoch 41/300\n",
      " - 0s - loss: 1.9422 - acc: 0.3636\n",
      "Epoch 42/300\n",
      " - 0s - loss: 1.9232 - acc: 0.3636\n",
      "Epoch 43/300\n",
      " - 0s - loss: 1.9037 - acc: 0.3636\n",
      "Epoch 44/300\n",
      " - 0s - loss: 1.8836 - acc: 0.3636\n",
      "Epoch 45/300\n",
      " - 0s - loss: 1.8630 - acc: 0.3636\n",
      "Epoch 46/300\n",
      " - 0s - loss: 1.8419 - acc: 0.3636\n",
      "Epoch 47/300\n",
      " - 0s - loss: 1.8202 - acc: 0.4545\n",
      "Epoch 48/300\n",
      " - 0s - loss: 1.7979 - acc: 0.4545\n",
      "Epoch 49/300\n",
      " - 0s - loss: 1.7752 - acc: 0.4545\n",
      "Epoch 50/300\n",
      " - 0s - loss: 1.7520 - acc: 0.4545\n",
      "Epoch 51/300\n",
      " - 0s - loss: 1.7285 - acc: 0.4545\n",
      "Epoch 52/300\n",
      " - 0s - loss: 1.7047 - acc: 0.4545\n",
      "Epoch 53/300\n",
      " - 0s - loss: 1.6807 - acc: 0.5455\n",
      "Epoch 54/300\n",
      " - 0s - loss: 1.6566 - acc: 0.6364\n",
      "Epoch 55/300\n",
      " - 0s - loss: 1.6325 - acc: 0.6364\n",
      "Epoch 56/300\n",
      " - 0s - loss: 1.6085 - acc: 0.6364\n",
      "Epoch 57/300\n",
      " - 0s - loss: 1.5847 - acc: 0.6364\n",
      "Epoch 58/300\n",
      " - 0s - loss: 1.5612 - acc: 0.6364\n",
      "Epoch 59/300\n",
      " - 0s - loss: 1.5380 - acc: 0.7273\n",
      "Epoch 60/300\n",
      " - 0s - loss: 1.5152 - acc: 0.8182\n",
      "Epoch 61/300\n",
      " - 0s - loss: 1.4928 - acc: 0.8182\n",
      "Epoch 62/300\n",
      " - 0s - loss: 1.4707 - acc: 0.7273\n",
      "Epoch 63/300\n",
      " - 0s - loss: 1.4489 - acc: 0.7273\n",
      "Epoch 64/300\n",
      " - 0s - loss: 1.4274 - acc: 0.7273\n",
      "Epoch 65/300\n",
      " - 0s - loss: 1.4062 - acc: 0.7273\n",
      "Epoch 66/300\n",
      " - 0s - loss: 1.3853 - acc: 0.7273\n",
      "Epoch 67/300\n",
      " - 0s - loss: 1.3647 - acc: 0.7273\n",
      "Epoch 68/300\n",
      " - 0s - loss: 1.3443 - acc: 0.7273\n",
      "Epoch 69/300\n",
      " - 0s - loss: 1.3242 - acc: 0.7273\n",
      "Epoch 70/300\n",
      " - 0s - loss: 1.3043 - acc: 0.7273\n",
      "Epoch 71/300\n",
      " - 0s - loss: 1.2848 - acc: 0.7273\n",
      "Epoch 72/300\n",
      " - 0s - loss: 1.2655 - acc: 0.7273\n",
      "Epoch 73/300\n",
      " - 0s - loss: 1.2464 - acc: 0.7273\n",
      "Epoch 74/300\n",
      " - 0s - loss: 1.2277 - acc: 0.8182\n",
      "Epoch 75/300\n",
      " - 0s - loss: 1.2091 - acc: 0.8182\n",
      "Epoch 76/300\n",
      " - 0s - loss: 1.1909 - acc: 0.8182\n",
      "Epoch 77/300\n",
      " - 0s - loss: 1.1728 - acc: 0.8182\n",
      "Epoch 78/300\n",
      " - 0s - loss: 1.1550 - acc: 0.8182\n",
      "Epoch 79/300\n",
      " - 0s - loss: 1.1375 - acc: 0.8182\n",
      "Epoch 80/300\n",
      " - 0s - loss: 1.1201 - acc: 0.8182\n",
      "Epoch 81/300\n",
      " - 0s - loss: 1.1030 - acc: 0.9091\n",
      "Epoch 82/300\n",
      " - 0s - loss: 1.0861 - acc: 0.9091\n",
      "Epoch 83/300\n",
      " - 0s - loss: 1.0694 - acc: 0.9091\n",
      "Epoch 84/300\n",
      " - 0s - loss: 1.0529 - acc: 0.9091\n",
      "Epoch 85/300\n",
      " - 0s - loss: 1.0365 - acc: 0.8182\n",
      "Epoch 86/300\n",
      " - 0s - loss: 1.0204 - acc: 0.8182\n",
      "Epoch 87/300\n",
      " - 0s - loss: 1.0044 - acc: 0.9091\n",
      "Epoch 88/300\n",
      " - 0s - loss: 0.9886 - acc: 0.9091\n",
      "Epoch 89/300\n",
      " - 0s - loss: 0.9729 - acc: 0.9091\n",
      "Epoch 90/300\n",
      " - 0s - loss: 0.9575 - acc: 0.9091\n",
      "Epoch 91/300\n",
      " - 0s - loss: 0.9422 - acc: 0.9091\n",
      "Epoch 92/300\n",
      " - 0s - loss: 0.9271 - acc: 0.9091\n",
      "Epoch 93/300\n",
      " - 0s - loss: 0.9122 - acc: 0.9091\n",
      "Epoch 94/300\n",
      " - 0s - loss: 0.8974 - acc: 0.9091\n",
      "Epoch 95/300\n",
      " - 0s - loss: 0.8828 - acc: 0.9091\n",
      "Epoch 96/300\n",
      " - 0s - loss: 0.8684 - acc: 0.9091\n",
      "Epoch 97/300\n",
      " - 0s - loss: 0.8542 - acc: 0.9091\n",
      "Epoch 98/300\n",
      " - 0s - loss: 0.8402 - acc: 0.9091\n",
      "Epoch 99/300\n",
      " - 0s - loss: 0.8265 - acc: 0.9091\n",
      "Epoch 100/300\n",
      " - 0s - loss: 0.8128 - acc: 0.9091\n",
      "Epoch 101/300\n",
      " - 0s - loss: 0.7993 - acc: 0.9091\n",
      "Epoch 102/300\n",
      " - 0s - loss: 0.7860 - acc: 0.9091\n",
      "Epoch 103/300\n",
      " - 0s - loss: 0.7730 - acc: 0.9091\n",
      "Epoch 104/300\n",
      " - 0s - loss: 0.7601 - acc: 0.9091\n",
      "Epoch 105/300\n",
      " - 0s - loss: 0.7474 - acc: 0.9091\n",
      "Epoch 106/300\n",
      " - 0s - loss: 0.7349 - acc: 0.9091\n",
      "Epoch 107/300\n",
      " - 0s - loss: 0.7226 - acc: 0.9091\n",
      "Epoch 108/300\n",
      " - 0s - loss: 0.7104 - acc: 0.9091\n",
      "Epoch 109/300\n",
      " - 0s - loss: 0.6985 - acc: 0.9091\n",
      "Epoch 110/300\n",
      " - 0s - loss: 0.6867 - acc: 0.9091\n",
      "Epoch 111/300\n",
      " - 0s - loss: 0.6751 - acc: 0.9091\n",
      "Epoch 112/300\n",
      " - 0s - loss: 0.6637 - acc: 0.9091\n",
      "Epoch 113/300\n",
      " - 0s - loss: 0.6525 - acc: 0.9091\n",
      "Epoch 114/300\n",
      " - 0s - loss: 0.6415 - acc: 0.9091\n",
      "Epoch 115/300\n",
      " - 0s - loss: 0.6306 - acc: 0.9091\n",
      "Epoch 116/300\n",
      " - 0s - loss: 0.6199 - acc: 0.9091\n",
      "Epoch 117/300\n",
      " - 0s - loss: 0.6094 - acc: 0.9091\n",
      "Epoch 118/300\n",
      " - 0s - loss: 0.5991 - acc: 0.9091\n",
      "Epoch 119/300\n",
      " - 0s - loss: 0.5889 - acc: 0.9091\n",
      "Epoch 120/300\n",
      " - 0s - loss: 0.5790 - acc: 0.9091\n",
      "Epoch 121/300\n",
      " - 0s - loss: 0.5691 - acc: 0.9091\n",
      "Epoch 122/300\n",
      " - 0s - loss: 0.5595 - acc: 0.9091\n",
      "Epoch 123/300\n",
      " - 0s - loss: 0.5500 - acc: 0.9091\n",
      "Epoch 124/300\n",
      " - 0s - loss: 0.5407 - acc: 0.9091\n",
      "Epoch 125/300\n",
      " - 0s - loss: 0.5315 - acc: 0.9091\n",
      "Epoch 126/300\n",
      " - 0s - loss: 0.5225 - acc: 0.9091\n",
      "Epoch 127/300\n",
      " - 0s - loss: 0.5136 - acc: 0.9091\n",
      "Epoch 128/300\n",
      " - 0s - loss: 0.5049 - acc: 0.9091\n",
      "Epoch 129/300\n",
      " - 0s - loss: 0.4964 - acc: 0.9091\n",
      "Epoch 130/300\n",
      " - 0s - loss: 0.4880 - acc: 0.9091\n",
      "Epoch 131/300\n",
      " - 0s - loss: 0.4797 - acc: 0.9091\n",
      "Epoch 132/300\n",
      " - 0s - loss: 0.4716 - acc: 1.0000\n",
      "Epoch 133/300\n",
      " - 0s - loss: 0.4637 - acc: 1.0000\n",
      "Epoch 134/300\n",
      " - 0s - loss: 0.4559 - acc: 1.0000\n",
      "Epoch 135/300\n",
      " - 0s - loss: 0.4482 - acc: 1.0000\n",
      "Epoch 136/300\n",
      " - 0s - loss: 0.4407 - acc: 1.0000\n",
      "Epoch 137/300\n",
      " - 0s - loss: 0.4333 - acc: 1.0000\n",
      "Epoch 138/300\n",
      " - 0s - loss: 0.4260 - acc: 1.0000\n",
      "Epoch 139/300\n",
      " - 0s - loss: 0.4189 - acc: 1.0000\n",
      "Epoch 140/300\n",
      " - 0s - loss: 0.4119 - acc: 1.0000\n",
      "Epoch 141/300\n",
      " - 0s - loss: 0.4050 - acc: 1.0000\n",
      "Epoch 142/300\n",
      " - 0s - loss: 0.3983 - acc: 1.0000\n",
      "Epoch 143/300\n",
      " - 0s - loss: 0.3916 - acc: 1.0000\n",
      "Epoch 144/300\n",
      " - 0s - loss: 0.3851 - acc: 1.0000\n",
      "Epoch 145/300\n",
      " - 0s - loss: 0.3788 - acc: 1.0000\n",
      "Epoch 146/300\n",
      " - 0s - loss: 0.3725 - acc: 1.0000\n",
      "Epoch 147/300\n",
      " - 0s - loss: 0.3664 - acc: 1.0000\n",
      "Epoch 148/300\n",
      " - 0s - loss: 0.3604 - acc: 1.0000\n",
      "Epoch 149/300\n",
      " - 0s - loss: 0.3544 - acc: 1.0000\n",
      "Epoch 150/300\n",
      " - 0s - loss: 0.3487 - acc: 1.0000\n",
      "Epoch 151/300\n",
      " - 0s - loss: 0.3430 - acc: 1.0000\n",
      "Epoch 152/300\n",
      " - 0s - loss: 0.3374 - acc: 1.0000\n",
      "Epoch 153/300\n",
      " - 0s - loss: 0.3319 - acc: 1.0000\n",
      "Epoch 154/300\n",
      " - 0s - loss: 0.3266 - acc: 1.0000\n",
      "Epoch 155/300\n",
      " - 0s - loss: 0.3213 - acc: 1.0000\n",
      "Epoch 156/300\n",
      " - 0s - loss: 0.3161 - acc: 1.0000\n",
      "Epoch 157/300\n",
      " - 0s - loss: 0.3111 - acc: 1.0000\n",
      "Epoch 158/300\n",
      " - 0s - loss: 0.3061 - acc: 1.0000\n",
      "Epoch 159/300\n",
      " - 0s - loss: 0.3012 - acc: 1.0000\n",
      "Epoch 160/300\n",
      " - 0s - loss: 0.2964 - acc: 1.0000\n",
      "Epoch 161/300\n",
      " - 0s - loss: 0.2917 - acc: 1.0000\n",
      "Epoch 162/300\n",
      " - 0s - loss: 0.2871 - acc: 1.0000\n",
      "Epoch 163/300\n",
      " - 0s - loss: 0.2825 - acc: 1.0000\n",
      "Epoch 164/300\n",
      " - 0s - loss: 0.2781 - acc: 1.0000\n",
      "Epoch 165/300\n",
      " - 0s - loss: 0.2737 - acc: 1.0000\n",
      "Epoch 166/300\n",
      " - 0s - loss: 0.2694 - acc: 1.0000\n",
      "Epoch 167/300\n",
      " - 0s - loss: 0.2652 - acc: 1.0000\n",
      "Epoch 168/300\n",
      " - 0s - loss: 0.2611 - acc: 1.0000\n",
      "Epoch 169/300\n",
      " - 0s - loss: 0.2570 - acc: 1.0000\n",
      "Epoch 170/300\n",
      " - 0s - loss: 0.2530 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171/300\n",
      " - 0s - loss: 0.2491 - acc: 1.0000\n",
      "Epoch 172/300\n",
      " - 0s - loss: 0.2452 - acc: 1.0000\n",
      "Epoch 173/300\n",
      " - 0s - loss: 0.2414 - acc: 1.0000\n",
      "Epoch 174/300\n",
      " - 0s - loss: 0.2377 - acc: 1.0000\n",
      "Epoch 175/300\n",
      " - 0s - loss: 0.2340 - acc: 1.0000\n",
      "Epoch 176/300\n",
      " - 0s - loss: 0.2304 - acc: 1.0000\n",
      "Epoch 177/300\n",
      " - 0s - loss: 0.2268 - acc: 1.0000\n",
      "Epoch 178/300\n",
      " - 0s - loss: 0.2233 - acc: 1.0000\n",
      "Epoch 179/300\n",
      " - 0s - loss: 0.2199 - acc: 1.0000\n",
      "Epoch 180/300\n",
      " - 0s - loss: 0.2165 - acc: 1.0000\n",
      "Epoch 181/300\n",
      " - 0s - loss: 0.2132 - acc: 1.0000\n",
      "Epoch 182/300\n",
      " - 0s - loss: 0.2099 - acc: 1.0000\n",
      "Epoch 183/300\n",
      " - 0s - loss: 0.2067 - acc: 1.0000\n",
      "Epoch 184/300\n",
      " - 0s - loss: 0.2035 - acc: 1.0000\n",
      "Epoch 185/300\n",
      " - 0s - loss: 0.2004 - acc: 1.0000\n",
      "Epoch 186/300\n",
      " - 0s - loss: 0.1974 - acc: 1.0000\n",
      "Epoch 187/300\n",
      " - 0s - loss: 0.1943 - acc: 1.0000\n",
      "Epoch 188/300\n",
      " - 0s - loss: 0.1914 - acc: 1.0000\n",
      "Epoch 189/300\n",
      " - 0s - loss: 0.1885 - acc: 1.0000\n",
      "Epoch 190/300\n",
      " - 0s - loss: 0.1856 - acc: 1.0000\n",
      "Epoch 191/300\n",
      " - 0s - loss: 0.1827 - acc: 1.0000\n",
      "Epoch 192/300\n",
      " - 0s - loss: 0.1800 - acc: 1.0000\n",
      "Epoch 193/300\n",
      " - 0s - loss: 0.1772 - acc: 1.0000\n",
      "Epoch 194/300\n",
      " - 0s - loss: 0.1745 - acc: 1.0000\n",
      "Epoch 195/300\n",
      " - 0s - loss: 0.1718 - acc: 1.0000\n",
      "Epoch 196/300\n",
      " - 0s - loss: 0.1692 - acc: 1.0000\n",
      "Epoch 197/300\n",
      " - 0s - loss: 0.1666 - acc: 1.0000\n",
      "Epoch 198/300\n",
      " - 0s - loss: 0.1641 - acc: 1.0000\n",
      "Epoch 199/300\n",
      " - 0s - loss: 0.1616 - acc: 1.0000\n",
      "Epoch 200/300\n",
      " - 0s - loss: 0.1591 - acc: 1.0000\n",
      "Epoch 201/300\n",
      " - 0s - loss: 0.1567 - acc: 1.0000\n",
      "Epoch 202/300\n",
      " - 0s - loss: 0.1543 - acc: 1.0000\n",
      "Epoch 203/300\n",
      " - 0s - loss: 0.1520 - acc: 1.0000\n",
      "Epoch 204/300\n",
      " - 0s - loss: 0.1497 - acc: 1.0000\n",
      "Epoch 205/300\n",
      " - 0s - loss: 0.1474 - acc: 1.0000\n",
      "Epoch 206/300\n",
      " - 0s - loss: 0.1451 - acc: 1.0000\n",
      "Epoch 207/300\n",
      " - 0s - loss: 0.1429 - acc: 1.0000\n",
      "Epoch 208/300\n",
      " - 0s - loss: 0.1408 - acc: 1.0000\n",
      "Epoch 209/300\n",
      " - 0s - loss: 0.1386 - acc: 1.0000\n",
      "Epoch 210/300\n",
      " - 0s - loss: 0.1365 - acc: 1.0000\n",
      "Epoch 211/300\n",
      " - 0s - loss: 0.1345 - acc: 1.0000\n",
      "Epoch 212/300\n",
      " - 0s - loss: 0.1324 - acc: 1.0000\n",
      "Epoch 213/300\n",
      " - 0s - loss: 0.1304 - acc: 1.0000\n",
      "Epoch 214/300\n",
      " - 0s - loss: 0.1284 - acc: 1.0000\n",
      "Epoch 215/300\n",
      " - 0s - loss: 0.1265 - acc: 1.0000\n",
      "Epoch 216/300\n",
      " - 0s - loss: 0.1246 - acc: 1.0000\n",
      "Epoch 217/300\n",
      " - 0s - loss: 0.1227 - acc: 1.0000\n",
      "Epoch 218/300\n",
      " - 0s - loss: 0.1209 - acc: 1.0000\n",
      "Epoch 219/300\n",
      " - 0s - loss: 0.1190 - acc: 1.0000\n",
      "Epoch 220/300\n",
      " - 0s - loss: 0.1173 - acc: 1.0000\n",
      "Epoch 221/300\n",
      " - 0s - loss: 0.1155 - acc: 1.0000\n",
      "Epoch 222/300\n",
      " - 0s - loss: 0.1138 - acc: 1.0000\n",
      "Epoch 223/300\n",
      " - 0s - loss: 0.1121 - acc: 1.0000\n",
      "Epoch 224/300\n",
      " - 0s - loss: 0.1104 - acc: 1.0000\n",
      "Epoch 225/300\n",
      " - 0s - loss: 0.1087 - acc: 1.0000\n",
      "Epoch 226/300\n",
      " - 0s - loss: 0.1071 - acc: 1.0000\n",
      "Epoch 227/300\n",
      " - 0s - loss: 0.1055 - acc: 1.0000\n",
      "Epoch 228/300\n",
      " - 0s - loss: 0.1040 - acc: 1.0000\n",
      "Epoch 229/300\n",
      " - 0s - loss: 0.1024 - acc: 1.0000\n",
      "Epoch 230/300\n",
      " - 0s - loss: 0.1009 - acc: 1.0000\n",
      "Epoch 231/300\n",
      " - 0s - loss: 0.0994 - acc: 1.0000\n",
      "Epoch 232/300\n",
      " - 0s - loss: 0.0980 - acc: 1.0000\n",
      "Epoch 233/300\n",
      " - 0s - loss: 0.0965 - acc: 1.0000\n",
      "Epoch 234/300\n",
      " - 0s - loss: 0.0951 - acc: 1.0000\n",
      "Epoch 235/300\n",
      " - 0s - loss: 0.0937 - acc: 1.0000\n",
      "Epoch 236/300\n",
      " - 0s - loss: 0.0924 - acc: 1.0000\n",
      "Epoch 237/300\n",
      " - 0s - loss: 0.0910 - acc: 1.0000\n",
      "Epoch 238/300\n",
      " - 0s - loss: 0.0897 - acc: 1.0000\n",
      "Epoch 239/300\n",
      " - 0s - loss: 0.0884 - acc: 1.0000\n",
      "Epoch 240/300\n",
      " - 0s - loss: 0.0872 - acc: 1.0000\n",
      "Epoch 241/300\n",
      " - 0s - loss: 0.0859 - acc: 1.0000\n",
      "Epoch 242/300\n",
      " - 0s - loss: 0.0847 - acc: 1.0000\n",
      "Epoch 243/300\n",
      " - 0s - loss: 0.0835 - acc: 1.0000\n",
      "Epoch 244/300\n",
      " - 0s - loss: 0.0823 - acc: 1.0000\n",
      "Epoch 245/300\n",
      " - 0s - loss: 0.0811 - acc: 1.0000\n",
      "Epoch 246/300\n",
      " - 0s - loss: 0.0800 - acc: 1.0000\n",
      "Epoch 247/300\n",
      " - 0s - loss: 0.0789 - acc: 1.0000\n",
      "Epoch 248/300\n",
      " - 0s - loss: 0.0778 - acc: 1.0000\n",
      "Epoch 249/300\n",
      " - 0s - loss: 0.0767 - acc: 1.0000\n",
      "Epoch 250/300\n",
      " - 0s - loss: 0.0756 - acc: 1.0000\n",
      "Epoch 251/300\n",
      " - 0s - loss: 0.0746 - acc: 1.0000\n",
      "Epoch 252/300\n",
      " - 0s - loss: 0.0735 - acc: 1.0000\n",
      "Epoch 253/300\n",
      " - 0s - loss: 0.0725 - acc: 1.0000\n",
      "Epoch 254/300\n",
      " - 0s - loss: 0.0716 - acc: 1.0000\n",
      "Epoch 255/300\n",
      " - 0s - loss: 0.0706 - acc: 1.0000\n",
      "Epoch 256/300\n",
      " - 0s - loss: 0.0696 - acc: 1.0000\n",
      "Epoch 257/300\n",
      " - 0s - loss: 0.0687 - acc: 1.0000\n",
      "Epoch 258/300\n",
      " - 0s - loss: 0.0678 - acc: 1.0000\n",
      "Epoch 259/300\n",
      " - 0s - loss: 0.0669 - acc: 1.0000\n",
      "Epoch 260/300\n",
      " - 0s - loss: 0.0660 - acc: 1.0000\n",
      "Epoch 261/300\n",
      " - 0s - loss: 0.0651 - acc: 1.0000\n",
      "Epoch 262/300\n",
      " - 0s - loss: 0.0642 - acc: 1.0000\n",
      "Epoch 263/300\n",
      " - 0s - loss: 0.0634 - acc: 1.0000\n",
      "Epoch 264/300\n",
      " - 0s - loss: 0.0626 - acc: 1.0000\n",
      "Epoch 265/300\n",
      " - 0s - loss: 0.0618 - acc: 1.0000\n",
      "Epoch 266/300\n",
      " - 0s - loss: 0.0610 - acc: 1.0000\n",
      "Epoch 267/300\n",
      " - 0s - loss: 0.0602 - acc: 1.0000\n",
      "Epoch 268/300\n",
      " - 0s - loss: 0.0594 - acc: 1.0000\n",
      "Epoch 269/300\n",
      " - 0s - loss: 0.0587 - acc: 1.0000\n",
      "Epoch 270/300\n",
      " - 0s - loss: 0.0579 - acc: 1.0000\n",
      "Epoch 271/300\n",
      " - 0s - loss: 0.0572 - acc: 1.0000\n",
      "Epoch 272/300\n",
      " - 0s - loss: 0.0565 - acc: 1.0000\n",
      "Epoch 273/300\n",
      " - 0s - loss: 0.0558 - acc: 1.0000\n",
      "Epoch 274/300\n",
      " - 0s - loss: 0.0551 - acc: 1.0000\n",
      "Epoch 275/300\n",
      " - 0s - loss: 0.0544 - acc: 1.0000\n",
      "Epoch 276/300\n",
      " - 0s - loss: 0.0537 - acc: 1.0000\n",
      "Epoch 277/300\n",
      " - 0s - loss: 0.0531 - acc: 1.0000\n",
      "Epoch 278/300\n",
      " - 0s - loss: 0.0524 - acc: 1.0000\n",
      "Epoch 279/300\n",
      " - 0s - loss: 0.0518 - acc: 1.0000\n",
      "Epoch 280/300\n",
      " - 0s - loss: 0.0512 - acc: 1.0000\n",
      "Epoch 281/300\n",
      " - 0s - loss: 0.0506 - acc: 1.0000\n",
      "Epoch 282/300\n",
      " - 0s - loss: 0.0500 - acc: 1.0000\n",
      "Epoch 283/300\n",
      " - 0s - loss: 0.0494 - acc: 1.0000\n",
      "Epoch 284/300\n",
      " - 0s - loss: 0.0488 - acc: 1.0000\n",
      "Epoch 285/300\n",
      " - 0s - loss: 0.0482 - acc: 1.0000\n",
      "Epoch 286/300\n",
      " - 0s - loss: 0.0477 - acc: 1.0000\n",
      "Epoch 287/300\n",
      " - 0s - loss: 0.0471 - acc: 1.0000\n",
      "Epoch 288/300\n",
      " - 0s - loss: 0.0466 - acc: 1.0000\n",
      "Epoch 289/300\n",
      " - 0s - loss: 0.0460 - acc: 1.0000\n",
      "Epoch 290/300\n",
      " - 0s - loss: 0.0455 - acc: 1.0000\n",
      "Epoch 291/300\n",
      " - 0s - loss: 0.0450 - acc: 1.0000\n",
      "Epoch 292/300\n",
      " - 0s - loss: 0.0445 - acc: 1.0000\n",
      "Epoch 293/300\n",
      " - 0s - loss: 0.0440 - acc: 1.0000\n",
      "Epoch 294/300\n",
      " - 0s - loss: 0.0435 - acc: 1.0000\n",
      "Epoch 295/300\n",
      " - 0s - loss: 0.0430 - acc: 1.0000\n",
      "Epoch 296/300\n",
      " - 0s - loss: 0.0426 - acc: 1.0000\n",
      "Epoch 297/300\n",
      " - 0s - loss: 0.0421 - acc: 1.0000\n",
      "Epoch 298/300\n",
      " - 0s - loss: 0.0416 - acc: 1.0000\n",
      "Epoch 299/300\n",
      " - 0s - loss: 0.0412 - acc: 1.0000\n",
      "Epoch 300/300\n",
      " - 0s - loss: 0.0407 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2242e5e0cc0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "x\n",
    "model=Sequential()\n",
    "#[1,5,3,2,2] => [1.5, 2.5]\n",
    "model.add(Embedding(vocab_size, 10, input_length=max_len-1))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x,y, epochs=300, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "멀캠 교육생도 교육생도 있고 교육생도 교육생도 있다 교육생도 있다 지각하는 교육생도\n"
     ]
    }
   ],
   "source": [
    "def word_gen(model, t, curr_word, num):\n",
    "          #모델,토큰나이저,입력단어,개수\n",
    "        ini_word=curr_word\n",
    "        sent=''\n",
    "        for _ in range(num):\n",
    "            #print(t.word_index)\n",
    "            encoded=t.texts_to_sequences([curr_word])[0] #[3]\n",
    "            #print(encoded)\n",
    "            encoded=pad_sequences([encoded],maxlen=5,padding='pre') # [[0 0 0 0 3]]\n",
    "            #print(encoded)           \n",
    "            res=model.predict_classes(encoded)\n",
    "            #print(res) #4\n",
    "            for word, index in t.word_index.items():\n",
    "                if res==index: #동일한 단어를 찾았다면\n",
    "                    break\n",
    "            curr_word=curr_word+' '+word \n",
    "            sent=sent+' '+word            \n",
    "        sent=ini_word+sent\n",
    "        return sent      \n",
    "print(word_gen(model, t, '멀캠', 10))    \n",
    "#멀티캠퍼스에 라는 단어 뒤에 3개의 단어 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text=\"\"\"멀티캠퍼스에 있는 교육생들이 공부하고 있다\\n\n",
    "# 교육생들은 딥러닝을 하고있다\\n\n",
    "# 결석한 교육생도 있고 지각하는 교육생도 있다\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['articleID', 'articleWordCount', 'byline', 'documentType', 'headline',\n",
      "       'keywords', 'multimedia', 'newDesk', 'printPage', 'pubDate',\n",
      "       'sectionName', 'snippet', 'source', 'typeOfMaterial', 'webURL'],\n",
      "      dtype='object')\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"ArticlesApril2018.csv\")\n",
    "df\n",
    "print(df.columns)\n",
    "print(len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df) #1324\n",
    "df['headline'].isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1214"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline=[]\n",
    "headline.extend(list(df.headline.values))\n",
    "headline=[n for n in headline if n != 'Unknown']\n",
    "len(headline)\n",
    "#headline\n",
    "    #print(n)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1214\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "def mypreprocessing(s): #점 제거, 소문자 변환\n",
    "    #전처리된 문장을 리턴(변환된 문장 : nfl ...)\n",
    "    #return #전처리된 문장\n",
    "    return ''.join(ch for ch in s if ch not in punctuation).lower()\n",
    "    \n",
    "text=[mypreprocessing(x) for x in headline]\n",
    "#변환되기 전 원문장: N.F.L ...\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3619"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vocabulary 작성 (단어집합)\n",
    "t=Tokenizer()\n",
    "t.fit_on_texts(text)\n",
    "vocab_size=len(t.word_index)+1 #단어 집합의 크기 : 3620\n",
    "\n",
    "# x                      y\n",
    "# how to make          program\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[95, 263], [95, 263, 1100], [95, 263, 1100, 1101], [95, 263, 1100, 1101, 572], [95, 263, 1100, 1101, 572, 50], [95, 263, 1100, 1101, 572, 50, 7], [95, 263, 1100, 1101, 572, 50, 7, 2], [95, 263, 1100, 1101, 572, 50, 7, 2, 365], [95, 263, 1100, 1101, 572, 50, 7, 2, 365, 10], [95, 263, 1100, 1101, 572, 50, 7, 2, 365, 10, 1102], [96, 3], [96, 3, 1103], [96, 3, 1103, 2], [96, 3, 1103, 2, 14], [96, 3, 1103, 2, 14, 573], [96, 3, 1103, 2, 14, 573, 85], [96, 3, 1103, 2, 14, 573, 85, 1104], [96, 3, 1103, 2, 14, 573, 85, 1104, 366], [96, 3, 1103, 2, 14, 573, 85, 1104, 366, 367], [96, 3, 1103, 2, 14, 573, 85, 1104, 366, 367, 5]]\n"
     ]
    }
   ],
   "source": [
    "sequences=[]\n",
    "for line in text:  #1214개의 라인\n",
    "    encoded=t.texts_to_sequences([line])[0]\n",
    "    #print(len(encoded))\n",
    "    for i in range(1,len(encoded)):\n",
    "        sequence=encoded[:i+1]\n",
    "        sequences.append(sequence)\n",
    "print(sequences[:20])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(s) for s in sequences) #최대길이 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_gen(model, t, 'how', 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
