# 행렬식



행렬식은 평행사변형의 넓이와 같다

# 차원축소

- 차원의 저주: 차원이 올라가면 올라갈수록 저주받는다....?

굳이 고차원으로 올려서 데이터를 나타낼 필요가 없이, 낮은 차원으로 데이터를 표현해도 괜찮다.

d1이라는 데이터가 있고,

d1 = ( , , , , )

d1이 다섯개의 feature로 이루어져 있다고 합시다.

1. 동상 환자 수
2. 열사병 환자 수
3. 폭설 -> 교통사고 수
4. 제설 작업 비용
5. 폭우 -> 교통사고 수

변수 5개가 있는데, 굳이 5개를 전부 사용할 필요 없이 공통점을 묶어서 표현할 수 있지 않나 하는게 차원 축소입니다.

차원 축소의 다른 방법: t-SNE(이 것도 결국 PCA에서 파생된 방법으로 볼 수 있음)



# PCA(주성분 분석)



1. 방법1(x축 또는 y축으로 정사영 하기)

![1564624122666](C:\Users\user\Documents\machine learning\14일차\img\1.png)

2. 방법2(데이터의 변위가 가장 큰 순순서대로 나타낸 방향 벡터(주성분 벡터) 방향으로 정사영 하기)

![1564624208060](C:\Users\user\Documents\machine learning\14일차\img\2.png)



## 공분산 행렬

n개 data, d개의 features

![1564626074538](C:\Users\user\Documents\machine learning\14일차\img\3.png)

xT x의 i행 j열?

x data의 i번째 feature와 j번째 feature가 얼마나 닮았느냐?


$$
공분산 행렬, \sum = \frac{X^T X}{n}
$$

$$
AX - \lambda X = 0
$$

A: 공분산 행렬

lambda: 고유 값

X: 고유 벡터

### 공분산 행렬 적용 -> 선형 변환

ex.

![1564626448597](C:\Users\user\Documents\machine learning\14일차\img\4.png)

여기에 선형 변환을 하면, 길쭉한 모양의 점들의 집합으로 변환될 수도 있고 기울어진 모양의 점들의 집합으로 변환될 수도 있습니다.

### 고유벡터, 고유값의 의미

- 고유벡터: 행렬이 벡터에 작용하는 힘의 방향

공분산 행렬의 고유 벡터는 데이터가 어떤 방향으로 분산되어 있는가를 의미함.
$$
AX = \lambda X
$$
고유값 lambda의 의미는 여기에서는 상관계수를 의미함.

공분산행렬의 경우 고유값을 각 축에 따른 공분산 값



![1564626859598](C:\Users\user\Documents\machine learning\14일차\img\5.png)

고유값의 계산은 축의 개수만큼 lambda값이 나오기 때문에, 여러개의 vector가 나옵니다.

10차원 데이터를 차원축소 하려면

그 것을 eigen value와 eigen vector는 10개가 나옵니다.

그 것을 10개를 전부 사용하지 않고, 그 중에서 몇 개만 사용해서 표현을 하느냐가 차원 축소 기술이다. 라고 할 수 있습니다.

