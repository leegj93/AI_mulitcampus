데이터 통계에서 기술통계에서 가장 많이 사용되는 것이 중심 경향치라고 얘기하는데,

중심 경향치에 속하는 것들이 평균, 중앙값, 최빈수 등이 해당합니다.

중앙값은 극단 값에 의해서 평균값이 값이 왜곡되기 시작하니까,

그것을 보완하기 위한 값이 중앙값이죠


====통계========

정규분포, 선형변환, 표준 정규 분포

정규분포만 가지고 분석하기는 좀 힘이든 경우가 있어요

정규분포도 모양이 다양하기 때문에 표준정규분포가 있습니다.

정규분포를 선형변환(더하기,빼기, 곱하기 등)을 합니다. 그래서 그걸 표준 정규 분포를 만들어줍니다.

선형변환의 대표적인 것은 Z score가 있습니다.

$$
Z =  \frac{X - \bar X}{S_x}
$$


![íì¤ì ê·ë¶í¬ì ëí ì´ë¯¸ì§ ê²ìê²°ê³¼](C:\Users\user\Desktop\딥러닝 모델 설계\3일차\img\2)

표준편차가 Mu에 해당하고, 평균에서 1표준편차만큼 이동 되어있는 곳의 Z값이 1이 됨.

1sigma 안에 들어오는 것이 전체의 68%가 되고, 2sigma 안에 들어오는 것이 95%, 3sigma 안에 들어오는 것이 99.7%가 됨.

여기에서 조금만 더 나가면 `유의 수준`이라는 것도 도출이 됨

다시, 표준 정규분포는 표준편차가 1이고, 평균이 0인 정규분포를 말한다.

```python
# numpy에서는 a.mean()/a.std()로 하면 됨

# scipy에서는 stats라고 있음

# scikit-learn의 preprocessing 모델에 가보면, p_transform이라는 함수가 있어서 표준화가 됩니다.
```



```python
import numpy as np
data = np.random.randint(20, size=(6,5)) # 20까지의 범위 내에서 20 포함x

# 1. numpy 표준화
print(np.mean(data)) # 전체평균
data
# 표준화: (각 열의 데이터 - 각 열의 평균) / 각 열의 표준편차
# z = (x- mean()) / std()
print(np.mean(data, axis=0)) # 열단위, 행단위는 1
(data - np.mean(data, axis=0))/np.std(data, axis=0) #실수하지 않기!!

# 2. scipy 표준화
import scipy.stats as ss
std_data = ss.zscore(data)
std_data

# 3. sklearn 표준화
from sklearn.preprocessing import StandardScaler
std_data = StandardScaler().fit_transform(data)
std_data
```

하지만, 이러한 표준화는 이상치가 없다는 가정 하에 하는 것입니다.

이상치가 있다면 평균값이 이상치 때문에 바뀔 수가 있기 때문입니다.

따라서 이상치가 있는 경우라면, 이상치를 제거(box plot에서 확인 후)한 다음 표준화를 해주는 것이 좋습니다.

하지만, 이상치를 모두 제거하는 것이 안 좋은 경우도 있습니다.

이상치를 삭제하면 안되는 경우에 사용하는 것이 `robust scaler`입니다.

이상치에 영향을 크게 받는 것이 `평균, 표준편차`입니다.

이상치에 영향을 덜 받는 것이 `중앙값, IQR(Inter Quantile Range; Q3 - Q1)`입니다. Q2가 중앙값입니다. 

# Robust Scaler(이상치에 대해 견고함)

```python
# 중앙값(Q2), IQR(Q3 - Q1)
# robustscaler = x - median() / IQR()
import numpy as np
np.random.seed(73)
mu, sigma = 10, 3
x = mu + sigma*np.random.randn(100)
x
import matplotlib.pyplot as plt
plt.hist(x)
np.mean(x)

# 이상치 추가
x[95:100] = 100
x
np.mean(x)
np.std(x)
# 위 처럼 표준화 하면, 값이 너무 달라져서 제대로 모델링이 안됩니다.
plt.hist(x)
plt.hist(x, bins=np.arange(0, 101, 2))

print(x.shape)
x.reshape(-1, 1) # -1: 니가 알아서 계산해
# numpy에서는 -1로 사용하는데, 딥러닝에서는 None을 사용합니다.
# StandardScaler 사용
print(x.shape)
std_x = StandardScaler().fit_transform(x)
std_x
# 100으로 넣었던 애들이 4.3으로 바뀌어 있음
plt.hist(std_x)
std_x

std_o = std_x[std_x < 4]
plt.hist(std_o, bins=np.arange(-1, 1, 0.2))

# robustScaler
np.mean(x)
np.median(x)
Q1 = np.percentile(x, 25) # 25분위 수를 얻고 싶다.
Q3 = np.percentile(x, 75)
IQR = Q3 - Q1
rbs_x = RobustScaler.fit_transform(x)
np.median(rbs_x) # 0.0이 나옴
np.mean(rbs_x)
np.std(rbs_x) # 4.8정도 나옴

rbs_x 


```

여러분들이 객관적으로 평가를 하고 싶다 하는 경우에는 seed를 줘서 난수를 생성하는 것이 좋습니다.

이 것을 안주면 난수가 계속 바뀌기 때문입니다.

Scaler는 4가지가 있습니다.

`MinMaxScaler, StandardScaler, RobustScaler, maxAbsScaler`

# 정규화

(각 열의 요소 값 - 각 열의 최소값) / (각 열의 최대값 - 각 열의 최소값)

0 ~ 1 사이의 범위가 보장이 됩니다. 딥러닝 할 때는 정규화가 많이 사용되는 편이다.

```python
from sklearn.preprocessing import MinMaxScaler
x = np.array([[9, -9, 2],
         [5, 0, 1],
         [1, 10, 4],
         [4, 7, -2]])
x.min()
x.min(axis=0) # shape
x.min(axis=1)
(x-x.min(axis=0)) / (x.max(axis=0) - x.min(axis=0))

x_minmax = MinMaxScaler().fit_transform(x)
x_minmax

# 위의 결과와 같음
from sklearn.preprocessing import minmax_scale
minmax_scale(x, axis=0)
```

# 이항변수화

연속형 변수를 0 또는 1로 바꾸는 것

```python
from sklearn.preprocessing import Binarizer
x = np.array([[5, -3, 2],
             [7, -1, 0],
             [0, 9, 5]])

# 3을 기준(threshold)
Binarizer().fit(x) # default가 0.0으로 되어 있음
Binarizer(copy=True, threshold=3) # threshold값 조정
bnr = Binarizer(3).fit(x)
bnr.transform(x)
```

