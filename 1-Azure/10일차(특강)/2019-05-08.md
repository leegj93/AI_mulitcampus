MCT 자격증

Q. D Plus에서 매주 목요일마다 온라인으로 화상회의를 하는데, 어떻게 하시나요 ?

Q. 빌게이츠 옆에 아내분이 있던 사진이 있던데, 동종 

johns hopkins에서 학위를 취득받을 때, 비용은 어느정도 들었나요 ? R Programming

toastmasters. 각종 모임

적정기술 튜터링을 하고 있음

```
Sow a thought, and you reap and act;

sow an act, and you reap a habit;

sow an habit, and you reapa character;

sow a character, and you reap a destiny.
```

power point가 늦게 켜졌습니다. 문제가 뭔지 찾아야합니다. 껐다 켜라고 하는 것을, 원인을 찾으면 안꼈다켜도 해결할 수 있음

`컴퓨터가 느려요`. 어떻게 해야할까요 ? => `왜 느린지를 찾아야합니다`. SSD는 조각모음을 하면 수명이 짧아집니다. 

> `기본을 지켜야 합니다.`

`올바른 방향으로 노력하는 것이 중요합니다.`

라벤다 밭 가봐야지 정말 좋음

예전에 기자가 표본을 잡아서 세는 방법이 있었습니다. 이렇게 하는게 표본 추출입니다.

그림을 나누는 애가 있고, 나눠서 센 것을 합산 하는 것이 있습니다. 

![](img/1.png)

조지 가트너가 처음 빅데이터라는 말을 했습니다.

Bernard Marr: 5V (Gartner 3V)

1. 데이터의 양(volume)
2. 데이터의 속도(velocity)
3. 데이터의 다양성(variety)
4. 정확성(Veracity)
5. 가치(value)

`구글 포토`를 꼭 써 볼 것을 권장합니다.

`win + P` 를 누르면 프로젝트 화면이 나옵니다. 

`win + tab`을 누르면 현재 실행중인 창들이 나옴. 새로운 데스크탑을 생성하고, 

`win + ctrl + 화살표`

이런걸 공부하고 싶으면 `윈도우 단축키`를 검색하면 됩니다.

잠잘시간 더 확보하세요.

`구글 렌즈`가 있습니다, `구글 뉴스`


`구글 검색 잘하기`라고 검색하면 위의 것들을 알 수 있습니다.

`테드` 하루에 한편씩 보기

빅데이터를 잘하려면, 쓰레기 청소부터 잘해야 함.

빅데이터의 60% 이상은 데이터 청소작업입니다. (Q. 어떤 데이터를 수집해야합니까?)

캐글에서 가장 기본되는 자료가 타이타닉 자료인데, 여러분이 우선 배워야 하는 것은 에디터를 잘 사용하는 것입니다.

`정규식`이라는 것이 있습니다. [생활코딩 정규식](https://opentutorials.org/course/909/5142). 무조건 배우셔야 합니다.

엑셀의 최신 버전은 정규식 사용 안해도 Mr. Miss. 등을 구분할 수 있습니다.

분석해서 뭐가 좋은지 대답을 할 수 있어야 합니다.

타이타닉 데이터를 봤으면, 생존을 해야 합니다. 여러분이 어떻게 해야 생존할 수 있을지 분석할 수 있어야 하는게 매우 중요하고 사장한테 먹힙니다.

타이타닉에서 100% 같은 것은 믿지 말고, 80% 이상부터 보면됩니다.

구글 검색에서 `세바시 하영호`라고 검색해보면 좋습니다. 

data literacy(데이터를 엮어내는 능력; 신기한 개념이 당연한 개념으로 바뀌는데 걸리는 시간이 점점 짧아지고 있습니다.)

SNS같은 것을 꾸준히 한다면, 취업시에 인사팀에서 그것을 확인합니다. 안하는 것보다는 하는 것이 좋습니다.

`Noise Tube`검색해서 보기. `Person of Interest` 영화 재미있음. 빅데이터의 무서움을 알 수 있습니다.

한국에는 watcha가 있습니다. 별점 10개만 주면, 추천을 기가 막히게 잘 되어 있습니다.

- 구글: 맞춤광고
- 구글 번역: 문장 전체를 데이터베이스에
- 페이스북: 알 수도 있는 사람
- MLB: 머니볼
- 한국의 오픈메이트

`Datafication`: `분석하고 싶으면 모아라!`


용어 찾아서 검색해보기

`If you can not measure, you can not manage`

Peter Drucker가 했다고 주장되는 말, 논란에도 불구하고, (중요함)

## 통계

통계를 조금만 아는 사람은 mean(평균) 대신에 Median(중앙값)을 씁니다. 산술평균은 이상치 때문에 완전 엉망이 됩니다.

예를들어, 연봉에 관해서 사장 아들이 들어왔을 때 

평균에서 1표준편차만큼 떨어진 곳의 값은 65퍼센트, 1표준편차가 

`아웃라이어 - 말콤 글래드웰`이라는 책을 사서 보거나 빌려서 보세요

box plot

점으로 된 것이 outlier입니다. outlier를 그냥 삭제하면 안되고, outlier에 대한 분석을 따로 해야 합니다.

`새빨간 거짓말 - 통계` 안 읽어봐도 되고, 구글에서 그냥 검색해봐도 충분합니다.

상관 관계 != 인과 관계

상관관계는 `황새의 수`와 `신생아 수` 처럼 비슷한 그래프 모양을 가진 것은 상관관계가 대단이 높지만, 

인과관계와 헷갈리면 안된다.

독립 변수와의 상관성을 분석을 해서 서로 상관성이 잇으면 그거 `쓰면 안되는거에요`

## 구별해 내기: 인과 관계

속지 않기 위하여
1. 원본/출처를 확인
2. 어떻게 조사했나? 조사 방법 확인
3. 빠진 정보를 확인: 샘플 수, 유의 수준, 표준 편차
4. 상관관계 != 인과관계
5. 상식에 근거하여 생각

분석할 가치가 있을 때 분석을 해야 합니다.


이 교육을 들으면 그렇게 

오늘 그래서 결심을 하나 해야 합니다.


4차 산업혁명
===

2016년의 다보스포럼(세계 경제 포럼)

Ray Kuzwell의 2006, 특이점이 온다(The singularity is near). 이 책을 읽어보고, 토론해 보는 것이 좋음

4차 산업혁명의 핵심요소는 `connectivity + Intelligence`입니다.

`Industry 4.0` 독일에서 이런 용어를 2011년부터 쓰기 시작했음

ICT를 자동차 기계 산업에 접목

제조업을 서비스화 시키자.

IOT에서는 여러분도 하나의 THING이에요.

1970년대가 소프트웨어 전체의 암흑기였습니다.

시애틀이 비가 많이 오는 동네입니다. `Amazon GO`. Amazon 본사가 시애틀에 있음.`just walk out shopping`

만약 기업에 들어가서 선행연구를 하게 되면 논문을 참조해서 봐야합니다.

그 중에서도 좋은 논문은 영어 논문 중에서도

IEEE, ACM 마크가 있는 논문을 보셔야합니다.

데이터베이스를 하는 사람들이 모여서 만든 학회 이름이

`Database > LDB > VLDB`

대부분의 현실적인 비지니스 모델은 2진 분류 모델로 풀리기 때문에, 예측 모델을 만들되, 2진 분류로 임원들에게 제안하는 것이 좋습니다.

모델을 사용할 때 절대로 개인적인 경험이 들어가면 안됩니다. 회사의 발표하는 장소에서 선배가 `어 그 모델은 내가 해봤는데 안되던데?` 이런 경우에

객관적인 모델을 만들어서 해야 합니다.

이 계통에서 일하시려면 수학공부하셔야 하고, 두 번째로는 프로그래밍 언어를 하셔야합니다. `이 수학공식이 이 분야에서 왜 사용되는지 설득시킬 수 있어야 합니다.`

이게 객관성이고 우리가 만든 모델의 힘이 되는 겁니다. (Q. 수학공부는 어떤 공부를 해야 하나요?)

데이터 분석에서 가장 먼저 해야 하는 일은

`내가 알고싶은 미래가 뭔지 가장 빠르게 결정`

`Target Value 또는 클래스`라고 합니다.

그 사이에 있는 여러가지 값들을 `feature vector 또는 attribute`라고 합니다.

그리고 행 정보를 `instance`라고 합니다.

모든 데이터가 분석할 수 있는 데이터는 아닙니다. 

### Data Science & Data-Driven Decision Making(데이터 기반의 의사결정)

`Data-Driven Decision Making`

숫자로 바꿀 수 있으면 측정할 수 있고, 측정할 수 있으면 정렬할 수 있습니다.

우리가 수능 볼 때 가장 중요한 팩터는 국,영, 수. 이다. 이런 팩터를 결정하는 것이 DDD1입니다.

`좀 더 분석을 하면 정확도가 향상되는 결정사항`을 `DDD2`라고 합니다. 예를 들어 수학성적을 좀 더 올리면 더 좋은 대학으로 갈 수 있다.

기업에서 `데이터 자산`이란 것은 `분석할 데이터`, `분석할 사람`입니다.

데이터가 있는지 없는지 판단은 target value가 있는지 먼저 보는 것이고, 우리가 원하는 예측값이 있는지 없는지 봐야 합니다.

###

자동화 가능한 부분, 창의력이 필요한 부분이 어딘지를 아셔야합니다.

창의력이 필요한 부분은 데이터 전처리하는 부분입니다. 그래서 알고리즘을 모르면 이 분야 못 들어옵니다.

전처리에 따라서 결과가 정말 다른 결과가 나올 수 있습니다.

창의력이


Data Mining Algorithms(각 책 한권씩들이라서 )
- 분류와 계층확률 추정
  - Classification & Class Probability Estimation
  - 각 개인이 어느 계층에 속할 것인지 예측
  - 각 계층은 상호 배타적
  - 분류작업 시 개인이 어느 계층이 속할지 결정하는 모델생성
    - 점수화(Scoring) or 계층 추정 확률
- 회기분석
  - Regression (가치 추정 Value Estimation)
  - 각 개인에 대한 특징변수의 수치를 추정하거나 예측
  - 분류와 회귀분석
- 유사도 매칭(새로운 사람이 들어왔을 때, 이 사람이 어느 군집에 속할 것인지; 타겟 밸류가 있음)
  - Similarity Matching
  - 알려진 데이터에 기반해 비슷한 개인을 찾음
  - 고객에게 제품을 추천할 때 가장 많이 사용하는 방법
  - 분류, 회귀 분석, 군집화 같은 데이터마이닝 
- 군집화(데이터는 존재하는데, 타겟 벨류가 존재하지 않을 때)
- 동시발생 그룹화(e.x. 오늘 우리 마트에서 어느 물건들(set)이 가장 잘 풀릴 것 같니)
  - Co-occurrence Group
- 프로파일링(ex1. 금융쪽에서 침입탐지(intrusion detection); ex2. fraud detection)
  - Profiling
  - 행위기술(개인, 그룹 전체의 전형적인 행위 특징을 찾음)
  - 사기탐지, 비정상적인 시스템 침입 감시 같은 비정상 행위 탐지에서 사용
  - 현재 사용된 신용카드는 합당한 사용인가?
- 연결 예측(페이스북에서 쓰는 것)
- 데이터 축소
  - Data Reduction(feature selection; `feature selection`을 잘한다면 몸값 올리는데 도움이 됩니다.) 최신논문부터 역순으로 읽으시면 좋습니다.
- 인과모델링(그래프 이론과 확률이론 공부해야함; 내가 아침에 출근하려고 시동을 켰는데, 배터리나 기름부족을 의심을 하는게 합리적. 타이어가 펑크났나는 고려하지 않음. 따라서 확률표가 있으면 그것을 바탕으로 모델링을 할 수 있음; 인과모델이 있으면 어디에나 갖다 쓸 수 있음)
  - Causal Modeling

## 알고리즘

Abstract
- 예측 모델링(Predictive Modeling)
  - 감독 세분화(Supervised Segmentation)
  - 속성(Attribute)찾기
    - 데이터로 표현되는 객체의 정보를 전달하는 중요한 변수
    - 타겟 값 + 연관된 변수 찾기
- 예측 모델링 기법
  - 엔트로피(Entropy)
  - 의사 결정 나무(Decision Tree)

### Model, Induction, Prediction
- 모델
  - 어떤 목적 달성을 위해 실 세계를 단순하게 표현한 것
- 예측모델
  - 우리가 관심 있는 변수(타겟 값)을 예측하는 곡잇
  - 감독학습
    - 일련의 변수와 타겟 변수 간의 관계를 보여주는 모델을 생성하는 것
    - 확률함수로 특징을 입력 받아 타겟 변수 값을 추정
- 설명 모델링
  - 값의 추정이 아닌 어떤 현상이나 절차를 쉽게 설명
- 모델 유도(Model Induction): 데이터로부터 모델을 만드는 것. 데이터를 보지 않고 거꾸로 모델을 만들어가는 것도 있음. 수학적으로는 귀납법, 통계학에서는 범용규칙이라고 합니다.
- 감독 세분화 모델

### Entropy

```
entropy = -p1*log(p1) - p2*log(p2) - ...
p1, p2는 확률값입니다. 즉 앞의 -가 가지는 의미는 확률값이 1보다 작기 때문에 양수로 표현하기 때문에, -를 붙여주는 것입니다.
```
물리학에서는 엔트로피가 1에서 가장 안정한 것을 나타내기 위해서 표현하는 것임. 앞으로 우리는 엔트로피가 0에 가까울 수록 타겟벨류가 분류가 잘 되겠구나.

```
Information Gain은 정보증가량입니다. (타겟 벨류를 분류할 때 분류를 잘하는지의 기준)
Information Gain(Parents, children) = entropy(parents) - [p(c1) x entropy(c1) + p(c2) x entropy(c2) + ...]
인포메이션 게인은 크면 클 수록 좋은 것입니다. 확률이라는 것은 셀 수 있어야 구할 수 있습니다.
```
초보 분석가들이 가장 흔하게 하는 실수가 문자열을 처리할 수 없다는 것인데, 확률 모델에서는 counting할 수 있게 전처리를 해야 합니다.

셀 수 있어야 확률을 구할 수 있습니다.

확률 모델에서는 굳이 숫자가 안들어와도 문자열이더라도 셀 수 있기 때문에 데이터를 날릴 필요는 없음.

### Information Gain 구하기 예제.
```python
from math import *
p1 = 16/30
p2 = 14/30
P = -p1*log2(p1) - p2*log2(p2)

c1_1 = 12/13
c1_2 = 1/13
c1 = -c1_1*log2(c1_1) - c1_2*log2(c1_2)

c2_1 = 4/17
c2_2 = 13/17
c2 = -c2_1*log2(c1_1) - c2_2*log2(c1_2)

p(c1) = 13/30
p(c2) = 17/30

ans = P - [p(c1)*c1 + p(c2)*c2] # 전체 엔트로피에서 가중치를 곱한 각 엔트로피의 합을 빼준 값이 Information Gain
```

똑같은 알고리즘을 쓰더라도 전처리를 다르게 하면 전혀 다른 결과가 나옵니다.

weight라는 값이 나오면 앞으로 우리는 가중치라고 읽지만 `기울기`라고 속으로 생각해야하고, 실제로 값을 보정하는 의미로는 잘 사용되지 않습니다.

엔트로피 게인이 작은 것부터 지워나갈 수 있음. feature selection의 한 가지 단순한 방법이 됨.

segmentation을 시켜야 하는데, 데이터 셋을 조각내는데 지금보다는 조금 더 좋은 모양으로 조각이 나야되죠

지금 보다 조금 더 yes는 yes끼리 몰려 있고 No는 No끼리 몰려있어야 하는 것이 더 좋게 조각난거겠죠

`설명 모델링` 중에서 `디시젼 트리가 괜찮습니다.`

~을 보세요 ~ 만약

여러분들이 나중에 모듈을 잘 

데이터의 구조를 잘 모르겠어 ~ 이럴 때 가장 먼저 그럼 우리 디시젼 트리 한번 돌려볼까 ? 하는 것이 제일 먼저임.

모든 툴베이스 안에 다 들어가 있음.

정확한 알고리즘을 이해하고 있어야 성능 개선포인트를 찾을 수 있기 때문에, 타사와의 차이점을 갖게 되는 본인만의 알고리즘을 만들 수 있습니다.

같은 도메인의 다른 회사들은 같은 데이터를 가지고 있고, 같은 툴을 가지고 있기 때문에 큰 차별점이 없습니다. 본인만의 알고리즘으로 차별성을 만들어야 합니다.

스터디를 구성해서 뭔가를 잘하는 사람들이 세미나 해주는 형식으로 분위기를 잘 조성하는 것이 좋습니다.

공모전 나가보면 실력이 늘기때문에 팀을 짜서 공모전을 나가보시는 것이 좋습니다.