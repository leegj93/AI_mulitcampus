# 1. Overview

관사`는 그 문장을 이해하는데 의미가 없는 경우가 많아요

수치 같은 것도 `일반적으로 삭제`를 합니다.

어떠한 텍스트를 제거 할 것인지에 대한 고민을 많이 해야해요.

수치데이터를 그냥 지우면 안되죠. 중요한 데이터도 있죠

정관사, 부정관사도 대체적으로 지우긴하지만 의미가 있을 때도 있어요.

또 예를 들어서, 도널드 트럼프는 카드 트럼프와 다른 것이고

state라는 것은 

TF-IDF에 대해 중요하게 다룰 것이고, 어떤 단어가 문장에 주어져 있을 때

그 단어가 문장 속에서 얼마나 중요한 의미를 갖느냐

단어의 중요도를 수치로 나타내기 위한 대표적인 척도 중 하나가 TF-IDF라고 합니다.

TF-IDF는 그 문장에서 중요한 단어를 추출할 수 있고,

그 문장, 더 나아가서 문단, 단락에 대한 전체적인 의미를 이해하는 데에 도움을 줄 수 있어요

문장 속에서 중요한 단어가 무엇이냐, 이 문장이 긍정이냐 부정이냐 하는 것이 TF-IDF이고

서로 다른 두 문장이 주어져 있을 때

문장이 어느 정도 유사한 문장이냐 하는 것

두 문장이 유사한지를 측정하는 기법으로는 유클리디안 거리로 측정 하는게 있어요

bag of word(BOW)를 구성을 합니다. 단어들을 가방에 우선 집어 넣어요

그리고 각각의 단어들이 몇 개 있는지 

같은 단어들 끼리의 거리를 구합니다. 그리고 유사한 정도를 수치화 하는 기법이죠

단어가 10개가 있다 하면, 10차원의 공간이 만들어지는 거죠.

이게 대표적인 유클리디안 거리를 이용한 유사도 측정 방법이에요

그런데, 이 방법이 좀 문제점이 있어요

그래서 거리 보다는 각도를 보자. 각도를 고려하자, 두 벡터에 대한 사이각을 구합니다.

그걸 보고 코사인 유사도라고 합니다. 코사인 유사도를 이용해서 구하는게 더 정확하게 나와요

예를 들어 단어들이 굉장히 많이 나오는 경우에는 적절하지 않습니다.

그런 경우에는 코사인 유사도를 구해서 그 값을 토대로 얼마나 유사한지 비슷하지 않은지를 파악할 수 있죠

TF-IDF를 구할 때, 코사인 유사도를 쓰게된 이유가 표준화를 하지 않은 상태에서 분석을 했기 때문인데



긁어온 댓글이 부정일까 긍정일까하는게 있어요

a, the, an, this 같은 단어들의 경우에는 positive, negative한지 판단하는데에 중요한 변수가 아니라는 거죠

like, love, hate 같은 표현들이 있죠.

그런 표현들이 중요하지 방금 말씀드렸던 것들처럼 and같은 것은 중요하지 않습니다.

not 같은 경우에는 중요하죠. not, but.

이것을 어느정도 선에서 여러분들이 처리를 해야 할 것인가 정해주셔야 하는거에요

this, a, an이 무조건 중요하지 않은건 아니고 대체적으로 중요하지 않은 것이기 때문에

중요할 때도 있다구요

수치데이터는 일반적으로 삭제 하거든요. 수치가 중요한 경우도 있을 거란말이죠

예를들어 인구 조사를 한다고 했을 때, 통계청에 데이터가 없어서 데이터를 가져와서 분석한다고 하면

그 수치를 제거하면 뭘 가지고 분석을 합니까



특수문자도 보편적으로 제거를 하거든요. 특수문자도 제거해서 안되는 경우가 간혹 있겠죠

`원`기호 `달러`기호 같은 경우. 그런 데이터셋을 자주 볼 수 있거든요. 300$가 있다. 이런거 함부로 지우면 안되거든요.

이런 것들은 전처리에서 고려해야할 사항이다.

또 축약형 있죠. 어퍼스트로피 s

punctuation(구두점)을 제거해버리면 문장이 연결이 되어 버립니다. 이것도 고려해야할 부분이고.

영어 같은 경우에는 가령 be동사 is, am, 또한 과거형도 똑같은 거잖아요

같은걸로 취급을 해줘야 한다구요



이거말고도 더 있는데, 고려해야 할 것도 많아요. 

하지만 우리나라말이 영어보다 몇배는 더 힘들어요.

그것을 바탕으로 다음주에 개인 미니 프로젝트로 감성 분석기를 만들겠는데, 만드는데 있어서

중요한 기능들을 오늘 학습을 통해서 배우게 될 것 같습니다.

그 외에 불용어라고 해서, stop was? 라고 하는데, 많이 사용되지 않는 단어들을 불용어라고 해요

불용어 사전을 만든 사람의 주관적 기준 + 통계치에 대한 결과에 의해 만들어진 것이라서



어제 설치하고 로드했던 라이브러리 중에 TM라이브러리 있죠.

TM이라는 것은 텍스트에 대해서 여러가지 텍스트 마이닝 알고리즘을 통해서 텍스트를 이해하고 해석하기

위해 만든 것을 TM이라고 할 수 있는데, 한국어에 대한 처리를 하기 위한 패키지가 또 있어요

`KoNLP`를 이용해서 우리나라 말에 해당하는 자연어 처리를 할 수 있는

모듈들이 있는데 KoNLP라고 하는 것을 써야겠습니다.

그리고 TDM(Term Document Matrix)라고 하는 것이 있는데요,

단어, 문자열로 구성된 matrix를 만들어야 해요.

이걸 만드는게 자연어 처리의 기본이라고 할 수 있어요

TDM으로 부터 TF-IDF를 만들고

주어진 단어가 문서에서 어떠한 중요한 의미를 갖는지를 이해할 수 있게 된다.

# 2. 

말뭉치(Corpus) 텍스트 전처리

수집 -> 시각화 & 전처리 -> 분석 -> 시각화 -> 알고리즘 선택 -> 모델링 -> 모델 평가 -> 유지보수

```R
mytext <- c("software environment",
  "software  environment",
  "software\tenvironment")
mytext

# sapply # 입력: 리스트, 출력: 벡터
# lapply # 입력: 리스트, 출력: 리스트
library(stringr)
str_split(mytext, ' ') # 출력 결과 리스트로 나옵니다.

sapply(str_split(mytext, ' '), length)

lapply(str_split(mytext, ' '), length)

# 각 리스트 요소에 저장된 문자열의 길이
lapply(str_split(mytext, ' '), str_length)

# 공백 처리 과정: 공백이 얼마나 됐던 간에 공백 하나로 치환.
mytext.nowhitespace <- str_replace_all(mytext, "[[:space:]]+", " ")

sapply(str_split(mytext.nowhitespace, ' '), length)
sapply(str_split(mytext.nowhitespace, ' '), str_length)

# 대문자 구분 예제
mytext <- "The 45th President of the United
States, Donald Trump, states that he
knows how to play trump with the former president"
str_split(mytext, ' ')
myword <- unlist(str_extract_all(mytext, boundary("word"))) # 두 번째 인자에 boundary라는 함수를 쓸 수 있습니다.
# 순수하게 word라는 것을 추출해주는 거에요

# Trump 같은 경우에는 미리 전처리 해주는 것이 좋습니다.
myword <- str_replace(myword, "Trump", "Trump_unique_")
myword <- replace(myword, "States", "States_unique_")
tolower(myword)

mytext <- c("He is one of statisticians agreeing that R is the No. 1 statistical software.","He is one of statisticians agreeing that R is the No. one statistical software.")
str_split(mytext, " ")
mytext2 <- str_replace_all(mytext, "[[:digit:]]{1,}[[:space:]]{1,}","")
mytext2 <- str_split(mytext2, " ") # 
str_c(mytext2[[1]], collapse = " ") # 문자열을 연결하는 함수
str_c(mytext2[[2]], collapse = " ") # 위의 결과와는 달라졌습니다.

# mytext에서 숫자는 모두 _number_로 일괄 치환해보세요
mytext3 <- str_replace_all(mytext, "([[:digit:]])+", "_number_")
mytext3
mytext3 <- str_split(mytext3, " ")
mytext3

# 실습용 세번째 지문
mytext <- "Baek et al. (2014) argued that the state of default-setting is critical for people to protect their own personal privacy on the Internet."
# Baek et al. Baek 외에 더 있다는 의미임

str_split(mytext, " ") # 공백으로 구분하기
str_split(mytext, "\\.")
# 성씨 다음 et al. 이 오고, 이어서 (년도) 형식

# => "_reference_"로 일괄 치환하고자 함
mytext2 <- str_replace_all(mytext, "-", " ")
mytext2

mytext2 <- str_replace_all(mytext, "[[:upper:]]{1}[[:alpha:]]{1,}[[:space:]](et al\\.)[[:space:]]\\([[:digit:]]{4}\\)", "_reference_")
mytext2

#.을 제거, .뒤에 공백이 0개 이상인 경우에 대해서 제거

# 불용어 직접 등록 => mystopwords
mystopwords <- "(\\ba )|(\\ban )|(\\bthe )"
mytext <- c("she is an actor", "she is the actor")
str_replace_all(mytext, mystopwords, "")

# 불용어 사전을 만들면 좋지만, 학계에서 욕을 많이 먹을겁니다.
library("tm")
stopwords() # kind가 en으로 되어 있음, SMART도 있음
# 불용어에 해당하는 단어 목록.
stopwords("en") # 짧은 불용어 목록
stopwords("SMART") # 긴 불용어 목록

# 어근 동일화 처리... 가령 뒤에 s가 붙어있다, es가 붙어있다 하는 경우에는 동일하게 해야합니다.
# 어근 동일화, 시제 고려, 단수/복수 => 동일화
# 가고, 가다, 간, 가니, ... => 동일화

mystemmer.func <- function(mytext){
  mytext <- str_replace_all(mytext, "(\\bam )|(\\bare )|(\\bis )|(\\bwas )|(\\bwere )|(\\bbe )", "be ") # <where> <from> <to> 
  print(mytext)
}
mystemmer.func("I am a boy. You are a boy. He might be a boy.")
test <- c("I am a boy. You are a boy. He might be a boy.")
mytext.stem <- mystemmer.func(test)

# https://korean.go.kr/ 여기 안에 들어가면 유의어 사전이 있음
# 근데 완전하지가 않아서, 잘 만들면 칭송받을거에요
# 유의어 사전이 있긴 있는데, 돈 달라고 합니다.

table(str_split(test, " ")) # 어근 동일화 되기 전
table(str_split(mytext.stem, " ")) # 어근 동일화 한 후

# ngram
# 1gram은 의미 없고, 2gram(bi-gram)이나 3(tri)-gram
# n번 연이어 등장하는 단어들을 말함.
# 단어가 만약 2개가 연결되어 있다.
# bi gram, tri gram이라고 얘기하는거죠.

# 단어만 가지고는 문장을 이해하기가 어렵잖아요
# 그래서 n-gram이라는 것을 이용하게 되는데,
# 2-gram은 단어 두개를 가지고 이해하는 것

# 그렇다면 두 단어씩 쪼개보자.
# 두 단어를 가지고 하나의 단어로 처리하는 것이라고 보면 됩니다.
# 이게 어디에 쓰이는지 궁금할 수 있는데
# 이게 매우 중요해요
# 문장의 의미를 이해하는데 요긴하게 쓰입니다,
# n-gram + bayes 이론이 붙어요.
# 이걸가지고 문맥을 파악할 수 있거든요.
# bayes이론의 기반은 조건부 확률인데,
# 어떤 조건이 주어져 있는 상황에서 어떤 사건이 발생할 확률이 얼마나 되느냐 하는거거든요
# 구름이 겼는데, 비가 올 확률이 어떻게 되느냐
# 전제조건이 구름이 껴있는 상황이라는거죠
# 비가 올 확률이 어떻게 되느냐 하는거죠
# 어떤 방식으로 n gram과 bayes이론이 혼합이 될까요
# 앞에 단어가 어떤것이 왔을 때 그 다음에 어떤 것이 올지를
# 확률적으로 하는거죠!!

# 네이버나 다음같은 곳에서 검색할 때, 
# 방탄
#    소년단 => 0.5
#    복     => 0.1
#    ...    => 
# 내부적으로 이렇게 동작하는겁니다.

mytext <- "The United States comprises fifty states. In the United States, each state has its own laws. However, federal law overrides state law in the United States."
str_split(mytext, boundary("word")) # 단어 분리
str_split(mytext, boundary("character")) # 문자 분리
str_split(mytext, boundary("line_break")) # 순수하게 단어가 아니라 공백까지 합쳐서

myword <- unlist(str_split(mytext, boundary("word")))
length(table(myword)) # 서로 다른 단어들의 개수
sum(table(myword))

# states는 앞에 주어가 있으면 동사로 옵니다. 그래서 전처리 해줄 필요가 있습니다.
mytext.2gram <- str_replace_all(mytext, "\\bUnited States", "United_States")
myword2 <- str_extract_all(mytext.2gram, boundary("word")) # bi-gram으로 읽음
length(table(myword2)) # myword에 비해서 줄어든 것을 볼 수 있습니다.
sum(table(myword2))

# 연습문제1. mytext에 들어있는 단어들을 2단어씩 연결하여 출력
# The United
# United States
# States compries
# ...
# United States.
myword <- unlist(str_split(mytext, boundary("word")))
for(i in 1:(length(myword)-1)){
  print(paste(myword[c(i, i+1)], collapse = " "))
}
# gram이 앞에 있는 n이 커질 수록 동일한 단어를 찾기가 점점 어려워지는 거에요
# 10gram이라고 하면, 순서가 그대로 동일한게 있어야 한다는 의미인데,
# 그러기가 힘들겠죠. 4-gram도 사실 매우 드뭅니다.

# 조건부 확률
# P("comprises"|"The United States")? => the united states가 왔을 때 comprises가 올 확률
# Maximum 값이 뭐냐 ?
# 그 단어가 뭔지 구해서 descending해서 출력 해주는거죠
```



# 문자열 전처리 코드

```R
mycorpus <- mytempfunc(mycorpus,"-collar","collar")
mycorpus <- mytempfunc(mycorpus,"\\b((c|C)o-)","co")
mycorpus <- mytempfunc(mycorpus,"\\b((c|C)ross-)","cross")
mycorpus <- mytempfunc(mycorpus,"e\\.g\\.","for example")
mycorpus <- mytempfunc(mycorpus,"i\\.e\\.","that is")
mycorpus <- mytempfunc(mycorpus,"\\'s","")
mycorpus <- mytempfunc(mycorpus,"s’","s")
mycorpus <- mytempfunc(mycorpus,"ICD-","ICD")
mycorpus <- mytempfunc(mycorpus,"\\b((i|I)nter-)","inter")
mycorpus <- mytempfunc(mycorpus,"K-pop","Kpop")
mycorpus <- mytempfunc(mycorpus,"\\b((m|M)eta-)","meta")
mycorpus <- mytempfunc(mycorpus,"\\b((o|O)pt-)","opt")
mycorpus <- mytempfunc(mycorpus,"\\b((p|P)ost-)","post")
mycorpus <- mytempfunc(mycorpus,"-end","end")
mycorpus <- mytempfunc(mycorpus,"\\b((w|W)ithin-)","within")
mycorpus <- mytempfunc(mycorpus,"=","is equal to")
mycorpus <- mytempfunc(mycorpus,"and/or","and or")
mycorpus <- mytempfunc(mycorpus,"his/her","his her")
mycorpus <- mytempfunc(mycorpus,"-"," ")
```



# TF-IDF(Term Frequency - Inverse Document Frequency)

TF-IDF는 코퍼스에서 문서에서 등장하는 단어가 얼마만큼 중요한지를 알려주는 수치

[링크]([https://en.wikipedia.org/wiki/Tf%E2%80%93idf](https://en.wikipedia.org/wiki/Tf–idf))

|      |      | D1(TF) | D2(TF) | D3(TF) | D1(IDF)      | D2(IDF)      | D3(IDF)      |
| ---- | ---- | ------ | ------ | ------ | ------------ | ------------ | ------------ |
| t1   | love | 30     | 30     | 30     | log(3/3)=0   | log(3/3)=0   | log(3/3)=0   |
| t2   | like | 15     | 0      | 0      | log(3/1)=4.5 | log(3/1)=4.5 | log(3/1)=4.5 |
| t3   | hate | 0      | 15     | 0      | log(3/1)=4.5 | log(3/1)=4.5 | log(3/1)=4.5 |
| t4   | run  | 0      | 0      | 15     | log(3/1)=4.5 | log(3/1)=4.5 | log(3/1)=4.5 |

이 표를 TDM이라고 불러요. Term Document Matrix.

TDM을 만들어주는 패키지가 있어요. tm에 함수가 있어요.

행에는 term이 오고, 열에 document가 와요

DTM도 있어요. TDM/DTM ( 먼저나오면 행, 나중에 나오는게 열)
$$
IDF = log_{10} (코퍼스에 있는 문서의 수 / 특정 단어가 들어가 있는 문서의 수)
$$

$$
TF = 해당 문서에서 나온 단어들의 빈도
$$

$$
tfidf(t, d, D) = tf(t, d)*idf(t, D)
$$

따라서 D1을 보면 love는 30, like는 15*4.5>30, 따라서 like가 가장 중요한 문서입니다.

IDF에서는 분자는 상수이고, 분모가 바뀌기 때문에, 분모가 중요합니다.

D1만 보면 위의 love의 30과 like의 15 등이 TF입니다.

like는 D1, D2, D3에서 유독 D1에서 많이 나오는거에요

like가 그럼 엄청 중요한 것 처럼 보이는데,

TF를 구해서 IDF를 곱합니다.

내가 궁금한거는 D1이라는 문서부터 D3까지의 문서가 있죠

D1~D3 각 문서에서 가장 중요한 단어 ? 

D1:, D2:, D3:

실제 이걸로 주제를 찾아요

뉴스가 약 100여개가 있는데,

이 뉴스에서는 특히 이 단어가 중요하다. 하는 것이 핵심단어입니다.

하지만, 벌써 이게 만들어져 있어서 우리가 할 필요는 없습니다.

나중에 뭘 하게 되냐면, 이걸로 유클리디안 거리를 계산하게 되면,

뭔가 편협적으로 해석할 수 있는 여지가 있어요.

텍스트 처리에서는 어떤식으로 계산하냐면,

![](img/13.png)



![](img/14.png)

따라서 각도로 측정하는 코사인 유사도를 사용하게 됩니다.

```R
# 이거 다시 가공해야함.

mytext <- c("software environment",
  "software  environment",
  "software\tenvironment")
mytext

# sapply # 입력: 리스트, 출력: 벡터
# lapply # 입력: 리스트, 출력: 리스트
library(stringr)
str_split(mytext, ' ') # 출력 결과 리스트로 나옵니다.

sapply(str_split(mytext, ' '), length)

lapply(str_split(mytext, ' '), length)

# 각 리스트 요소에 저장된 문자열의 길이
lapply(str_split(mytext, ' '), str_length)

# 공백 처리 과정: 공백이 얼마나 됐던 간에 공백 하나로 치환.
mytext.nowhitespace <- str_replace_all(mytext, "[[:space:]]+", " ")

sapply(str_split(mytext.nowhitespace, ' '), length)
sapply(str_split(mytext.nowhitespace, ' '), str_length)

# 대문자 구분 예제
mytext <- "The 45th President of the United
States, Donald Trump, states that he
knows how to play trump with the former president"
str_split(mytext, ' ')
myword <- unlist(str_extract_all(mytext, boundary("word"))) # 두 번째 인자에 boundary라는 함수를 쓸 수 있습니다.
# 순수하게 word라는 것을 추출해주는 거에요

# Trump 같은 경우에는 미리 전처리 해주는 것이 좋습니다.
myword <- str_replace(myword, "Trump", "Trump_unique_")
myword <- replace(myword, "States", "States_unique_")
tolower(myword)

mytext <- c("He is one of statisticians agreeing that R is the No. 1 statistical software.","He is one of statisticians agreeing that R is the No. one statistical software.")
str_split(mytext, " ")
mytext2 <- str_replace_all(mytext, "[[:digit:]]{1,}[[:space:]]{1,}","")
mytext2 <- str_split(mytext2, " ") # 
str_c(mytext2[[1]], collapse = " ") # 문자열을 연결하는 함수
str_c(mytext2[[2]], collapse = " ") # 위의 결과와는 달라졌습니다.

# mytext에서 숫자는 모두 _number_로 일괄 치환해보세요
mytext3 <- str_replace_all(mytext, "([[:digit:]])+", "_number_")
mytext3
mytext3 <- str_split(mytext3, " ")
mytext3

# 실습용 세번째 지문
mytext <- "Baek et al. (2014) argued that the state of default-setting is critical for people to protect their own personal privacy on the Internet."
# Baek et al. Baek 외에 더 있다는 의미임

str_split(mytext, " ") # 공백으로 구분하기
str_split(mytext, "\\.")
# 성씨 다음 et al. 이 오고, 이어서 (년도) 형식

# => "_reference_"로 일괄 치환하고자 함
mytext2 <- str_replace_all(mytext, "-", " ")
mytext2

mytext2 <- str_replace_all(mytext, "[[:upper:]]{1}[[:alpha:]]{1,}[[:space:]](et al\\.)[[:space:]]\\([[:digit:]]{4}\\)", "_reference_")
mytext2

#.을 제거, .뒤에 공백이 0개 이상인 경우에 대해서 제거

# 불용어 직접 등록 => mystopwords
mystopwords <- "(\\ba )|(\\ban )|(\\bthe )"
mytext <- c("she is an actor", "she is the actor")
str_replace_all(mytext, mystopwords, "")

# 불용어 사전을 만들면 좋지만, 학계에서 욕을 많이 먹을겁니다.
library("tm")
stopwords() # kind가 en으로 되어 있음, SMART도 있음
# 불용어에 해당하는 단어 목록.
stopwords("en") # 짧은 불용어 목록
stopwords("SMART") # 긴 불용어 목록

# 어근 동일화 처리... 가령 뒤에 s가 붙어있다, es가 붙어있다 하는 경우에는 동일하게 해야합니다.
# 어근 동일화, 시제 고려, 단수/복수 => 동일화
# 가고, 가다, 간, 가니, ... => 동일화

mystemmer.func <- function(mytext){
  mytext <- str_replace_all(mytext, "(\\bam )|(\\bare )|(\\bis )|(\\bwas )|(\\bwere )|(\\bbe )", "be ") # <where> <from> <to> 
  print(mytext)
}
mystemmer.func("I am a boy. You are a boy. He might be a boy.")
test <- c("I am a boy. You are a boy. He might be a boy.")
mytext.stem <- mystemmer.func(test)

# https://korean.go.kr/ 여기 안에 들어가면 유의어 사전이 있음
# 근데 완전하지가 않아서, 잘 만들면 칭송받을거에요
# 유의어 사전이 있긴 있는데, 돈 달라고 합니다.

table(str_split(test, " ")) # 어근 동일화 되기 전
table(str_split(mytext.stem, " ")) # 어근 동일화 한 후

# ngram
# 1gram은 의미 없고, 2gram(bi-gram)이나 3(tri)-gram
# n번 연이어 등장하는 단어들을 말함.
# 단어가 만약 2개가 연결되어 있다.
# bi gram, tri gram이라고 얘기하는거죠.

# 단어만 가지고는 문장을 이해하기가 어렵잖아요
# 그래서 n-gram이라는 것을 이용하게 되는데,
# 2-gram은 단어 두개를 가지고 이해하는 것

# 그렇다면 두 단어씩 쪼개보자.
# 두 단어를 가지고 하나의 단어로 처리하는 것이라고 보면 됩니다.
# 이게 어디에 쓰이는지 궁금할 수 있는데
# 이게 매우 중요해요
# 문장의 의미를 이해하는데 요긴하게 쓰입니다,
# n-gram + bayes 이론이 붙어요.
# 이걸가지고 문맥을 파악할 수 있거든요.
# bayes이론의 기반은 조건부 확률인데,
# 어떤 조건이 주어져 있는 상황에서 어떤 사건이 발생할 확률이 얼마나 되느냐 하는거거든요
# 구름이 겼는데, 비가 올 확률이 어떻게 되느냐
# 전제조건이 구름이 껴있는 상황이라는거죠
# 비가 올 확률이 어떻게 되느냐 하는거죠
# 어떤 방식으로 n gram과 bayes이론이 혼합이 될까요
# 앞에 단어가 어떤것이 왔을 때 그 다음에 어떤 것이 올지를
# 확률적으로 하는거죠!!

# 네이버나 다음같은 곳에서 검색할 때, 
# 방탄
#    소년단 => 0.5
#    복     => 0.1
#    ...    => 
# 내부적으로 이렇게 동작하는겁니다.

mytext <- "The United States comprises fifty states. In the United States, each state has its own laws. However, federal law overrides state law in the United States."
str_split(mytext, boundary("word")) # 단어 분리
str_split(mytext, boundary("character")) # 문자 분리
str_split(mytext, boundary("line_break")) # 순수하게 단어가 아니라 공백까지 합쳐서

myword <- unlist(str_split(mytext, boundary("word")))
length(table(myword)) # 서로 다른 단어들의 개수
sum(table(myword))

# states는 앞에 주어가 있으면 동사로 옵니다. 그래서 전처리 해줄 필요가 있습니다.
mytext.2gram <- str_replace_all(mytext, "\\bUnited States", "United_States")
myword2 <- str_extract_all(mytext.2gram, boundary("word")) # bi-gram으로 읽음
length(table(myword2)) # myword에 비해서 줄어든 것을 볼 수 있습니다.
sum(table(myword2))

# 연습문제1. mytext에 들어있는 단어들을 2단어씩 연결하여 출력
# The United
# United States
# States compries
# ...
# United States.
myword <- unlist(str_split(mytext, boundary("word")))
for(i in 1:(length(myword)-1)){
  print(paste(myword[c(i, i+1)], collapse = " "))
}
# gram이 앞에 있는 n이 커질 수록 동일한 단어를 찾기가 점점 어려워지는 거에요
# 10gram이라고 하면, 순서가 그대로 동일한게 있어야 한다는 의미인데,
# 그러기가 힘들겠죠. 4-gram도 사실 매우 드뭅니다.

# 조건부 확률
# P("comprises"|"The United States")? => the united states가 왔을 때 comprises가 올 확률
# Maximum 값이 뭐냐 ?
# 그 단어가 뭔지 구해서 descending해서 출력 해주는거죠

library(tm)
my.text.location <- "papers/"

# 말뭉치 구성

mypaper <- VCorpus(DirSource(my.text.location))
# 싹 묶어서 말뭉치로 만들어버립니다.
summary(mypaper) # document들에 대한 내용을 볼 수 있습니다.
class(mypaper)
mypaper[[1]] # 990글자가 있다.
mypaper[[1]]$content # 내용물이 들어있음
mypaper[[2]]$content
mypaper[[3]]$content

mypaper[[2]]$meta # 메타 데이터
meta(mypaper[[2]], tag='author') <- "G. D. Hong"
mypaper[[2]]$meta # 저자가 홍 G. D.로 들어가 있음

# 단어_특수문자_단어 로 연결되어 있는 것을 추출해보기
# 코퍼스가 list형식이기 때문에 lapply를 사용
myfunc <- function(x){
  str_extract_all(x, "[[:alnum:]]{1,}[[:punct:]]{1,}[[:alnum:]]{1,}")
  # alnum은 alphabet과 number
  # 특수문자는 [[:punct:]]
}

mypuncts <- lapply(mypaper, myfunc)
mypuncts # character(0)라고 되있는 것은 만족하는 것이 없다는 것
# 일반적인 패턴이기 때문에 myfunc는 저장했다가 그대로 쓰셔도 돼요.
table(unlist(mypuncts)) # 빈도 추출
# 딱 보고 날리거나 처리하거나 하면 됩니다.

# 문제. 수치로된 자료를 추출해보세요. 전체문서에서
myfunc2 <- function(x){
  str_extract_all(x, "[[:digit:]]{1,}")
}
table(unlist(lapply(mypaper, myfunc2)))
# 숫자들은 별로 의미가 없네요.

# 날씨 뉴스에서 대전32도, 광주와 포항 33도, 대구 35도, 등 같은 문자는 중요한데,
# 물을 8잔 마셔야.. 에서 8은 중요하지 않습니다.
# 따라서 문맥에서 중요한지 아닌지가 결정됩니다.

# 대문자로 시작하는 것 찾아보세요.(고유명사)
myuppers <- function(x){
  str_extract_all(x, "[[:upper:]]{1}[[:alpha:]]{1,}")
  }
table(unlist(lapply(mypaper, myuppers)))
mycorpus <- tm_map(mypaper, removeNumbers) # remove 적고 탭하면 tm이 있는데 tm패키지라고 되어 있는 것 중에서 선택
mycorpus[[1]]$content # n = 해서 다음에 숫자가 사라졌음
removePunctuation("hello..,..word") # 특수문자가 제거됩니다.
install.packages("SnowballC") # 유용한 함수들이 많이 들어있음
library(SnowballC)
wordStem(c("learn", "learns", "learning", "learned"))
# help에 stemDocument라고 있습니다.
# cleaned <- tm_map(코퍼스변수, stemDocument) 이런식으로 어근을 제거함

# 한글은 형태소 분석기를 돌려서 품사로 나눠져요.
# 그 중에 필요로 하는 것만 뽑아내야 하는데, 성능이 별로 안 좋아요
# okt가 가장 성능이 좋은데, 그마저도 허접합니다.
# 만들기는 어렵고, 돈은 안되요
# okt는 파이썬 라이브러리에요.


# 문자열 전처리 코드.
mytempfunc <- function(myobj, oldexp, newexp){
  newobj <- tm_map(myobj, # 이전의 형식에 대해 새로운 형식으로 어떻게 바꿀지
         content_transformer( # content_transformer의 형식으로 함수를 줬음
           function(x, pattern){
             gsub(pattern, newexp, x)
             }
           ),
         oldexp)
} # 정규식을 함수 안에서 표현해야해서 복잡해진겁니다.
mycorpus <- mytempfunc(mycorpus,"-collar","collar")
mycorpus <- mytempfunc(mycorpus,"\\b((c|C)o-)","co")
mycorpus <- mytempfunc(mycorpus,"\\b((c|C)ross-)","cross")
mycorpus <- mytempfunc(mycorpus,"e\\.g\\.","for example")
mycorpus <- mytempfunc(mycorpus,"i\\.e\\.","that is")
mycorpus <- mytempfunc(mycorpus,"\\'s","")
mycorpus <- mytempfunc(mycorpus,"s’","s")
mycorpus <- mytempfunc(mycorpus,"ICD-","ICD")
mycorpus <- mytempfunc(mycorpus,"\\b((i|I)nter-)","inter")
mycorpus <- mytempfunc(mycorpus,"K-pop","Kpop")
mycorpus <- mytempfunc(mycorpus,"\\b((m|M)eta-)","meta")
mycorpus <- mytempfunc(mycorpus,"\\b((o|O)pt-)","opt")
mycorpus <- mytempfunc(mycorpus,"\\b((p|P)ost-)","post")
mycorpus <- mytempfunc(mycorpus,"-end","end")
mycorpus <- mytempfunc(mycorpus,"\\b((w|W)ithin-)","within")
mycorpus <- mytempfunc(mycorpus,"=","is equal to")
mycorpus <- mytempfunc(mycorpus,"and/or","and or")
mycorpus <- mytempfunc(mycorpus,"his/her","his her")
mycorpus <- mytempfunc(mycorpus,"-"," ")

mycorpus[[2]]$content

# 공란제거(양쪽 끝)
mycorpus <- tm_map(mycorpus, stripWhitespace) # 함수를 수행해주면 불필요한 white space를 제거해주는 역할을함

mycorpus[[2]]$content

# 대문자 -> 소문자 치환
mycorpus <- tm_map(mycorpus, stripWhitespace) # tolower
tm_map(mycorpus, tolower) # 이렇게 하면 안됨. tolower는 input이 string이 와야함
mycorpus <- tm_map(mycorpus, content_transformer(tolower)) # tm_map의 두번째 인자 자리의 함수는 corpus가 와야하기 때문에 변환
mycorpus <- tm_map(mycorpus, removeWords, words=stopwords('SMART'))
mycorpus[[1]]$content
tm_map(mycorpus, stemDocument, language="en") # 포터 스태밍 알고리즘을 사용함
# 스태밍 알고리즘이 포터 말고도 여러가지가 있는데
# 대표적으로 어근을 추출하여 동일화 하기 위한 알고리즘이 포터 스태밍 알고리즘임

# TF-IDF에 대해서 좀 배우셔야지
# 그 단어가 문장에서 얼마나 중요하냐에 대한걸 배우실 수 있거든요

# TF/IDF
# 문서*단어 행렬
# DTM
# 가로:문서, 세로:단어
dtm.e <- DocumentTermMatrix(mycorpus)
#TermDocumentMatrix()
dtm.e
inspect(dtm.e[1:3, 50:60]) # 용량이 매우 커지기 때문에 사이즈만 커집니다.

# 다음주 월요일에 TF-IDF 함수 추출하는거 해보고, 한국어 해볼게요.
# 한국어도 똑같아요. koNLP쓰는거 외엔 똑같아요.
# 다른 부분은 품사로 나눠진다는 점
# 품사 좀 하고나서 감성 분석으로 들어갈게요. 화/수 정도 되면 주어진 문장에서
# 감성 사전이 있거든요.
# -5 ~ +5까지 단어마다 값이 있어요. 그게 만들어져 있어서 그거 가지고 계산을 해요
# 감성사전이라고 해도, 주관적이겠죠. 그걸 일반적으로 많이 써요
# titanic이 있죠.

# 과제. 호칭 추출하기 => 수치화까지 해보기
# 연속형 변수같은 경우에는 주의할 점이 factor으로 잡혀야해요.
# Mr는 1, Mrs2, Miss는 3으로 했을 때 Mr + Mrs = Miss 아니잖아요.
# 그래서 factor로 잡혀야해요
```

